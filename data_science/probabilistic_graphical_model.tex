\chapter{Probabilistic Graphical Model}

View online \href{http://magizbox.com/training/probabilistic_graphical_models/site/}{http://magizbox.com/training/probabilistic_graphical_models/site/}

Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.

\section{Representation}

Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other.

These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.

\section{Foundation: Probability Theory}

he main focus of this book is on complex probability distributions. In this section we briefly review basic concepts from probability theory.

1 Probability Distributions
When we use the word â€œprobabilityâ€ in day-to-day life, we refer to a degree of confidence that an event of an uncertain nature will occur. For example, the weather report might say â€œthere is a low probability of light rain in the afternoon.â€ Probability theory deals with the formal foundations for discussing such estimates and the rules they should obey. Before we discuss the representation of probability, we need to define what the events are to which we want to assign a probability. These events might be dierent outcomes of throwing a die, the outcome of a horse race, the weather configurations in California, or the possible failures of a piece of machinery.

1.1 Event Spaces
event Formally, we define events by assuming that there is an agreed upon space of possible outcomes, outcome space which we denote by â„¦. For example, if we consider dice, we might set â„¦ = {1, 2, 3, 4, 5, 6}. In the case of a horse race, the space might be all possible orders of arrivals at the finish line, a much larger space.

measurable event In addition, we assume that there is a set of measurable events S to which we are willing to assign probabilities. Formally, each event Î± âˆˆ S is a subset of â„¦. In our die example, the event {6} represents the case where the die shows 6, and the event {1, 3, 5} represents the case of an odd outcome. In the horse-race example, we might consider the event â€œLucky Strike wins,â€ which contains all the outcomes in which the horse Lucky Strike is first. Probability theory requires that the event space satisfy three basic properties: â€¢ It contains the empty event âˆ…, and the trivial event â„¦. â€¢ It is closed under union. That is, if Î±, Î² âˆˆ S, then so is Î± âˆª Î². â€¢ It is closed under complementation. That is, if Î± âˆˆ S, then so is â„¦ âˆ’ Î±. The requirement that the event space is closed under union and complementation implies that it is also closed under other Boolean operations, such as intersection and set dierence.

1.2 Probability Distributions
Definition 2.1 A probability distribution P over (â„¦, S) is a mapping from events in S to real values that satisfies probability distribution the following conditions: â€¢ P(Î±) â‰¥ 0 for all Î± âˆˆ S. â€¢ P(â„¦) = 1. â€¢ If Î±, Î² âˆˆ S and Î± âˆ© Î² = âˆ…, then P(Î± âˆª Î²) = P(Î±) + P(Î²). The first condition states that probabilities are not negative. The second states that the â€œtrivial event,â€ which allows all possible outcomes, has the maximal possible probability of 1. The third condition states that the probability that one of two mutually disjoint events will occur is the sum of the probabilities of each event. These two conditions imply many other conditions. Of particular interest are P(âˆ…) = 0, and P(Î± âˆª Î²) = P(Î±) + P(Î²) âˆ’ P(Î± âˆ© Î²).

1.3 Interpretations of Probability
Before we continue to discuss probability distributions, we need to consider the interpretations that we might assign to them. Intuitively, the probability P(Î±) of an event Î± quantifies the degree of confidence that Î± will occur. If P(Î±) = 1, we are certain that one of the outcomes in Î± occurs, and if P(Î±) = 0, we consider all of them impossible. Other probability values represent options that lie between these two extremes. This description, however, does not provide an answer to what the numbers mean. There are two common interpretations for probabilities. frequentist The frequentist interpretation views probabilities as frequencies of events. More precisely, the interpretation probability of an event is the fraction of times the event occurs if we repeat the experiment indefinitely. For example, suppose we consider the outcome of a particular die roll. In this case, the statement P(Î±) = 0.3, for Î± = {1, 3, 5}, states that if we repeatedly roll this die and record the outcome, then the fraction of times the outcomes in Î± will occur is 0.3. More precisely, the limit of the sequence of fractions of outcomes in Î± in the first roll, the first two rolls, the first three rolls, . . ., the first n rolls, . . . is 0.3.

The frequentist interpretation gives probabilities a tangible semantics. When we discuss concrete physical systems (for example, dice, coin flips, and card games) we can envision how these frequencies are defined. It is also relatively straightforward to check that frequencies must satisfy the requirements of proper distributions. The frequentist interpretation fails, however, when we consider events such as â€œIt will rain tomorrow afternoon.â€ Although the time span of â€œTomorrow afternoonâ€ is somewhat ill defined, we expect it to occur exactly once. It is not clear how we define the frequencies of such events. Several attempts have been made to define the probability for such an event by finding a reference class reference class of similar events for which frequencies are well defined; however, none of them has proved entirely satisfactory. Thus, the frequentist approach does not provide a satisfactory interpretation for a statement such as â€œthe probability of rain tomorrow afternoon is 0.3.â€  An alternative interpretation views probabilities as subjective degrees of belief. Under subjective interpretation this interpretation, the statement P(Î±) = 0.3 represents a subjective statement about oneâ€™s own degree of belief that the event Î± will come about. Thus, the statement â€œthe probability of rain tomorrow afternoon is 50 percentâ€ tells us that in the opinion of the speaker, the chances of rain and no rain tomorrow afternoon are the same. Although tomorrow afternoon will occur only once, we can still have uncertainty about its outcome, and represent it using numbers (that is, probabilities). This description still does not resolve what exactly it means to hold a particular degree of belief. What stops a person from stating that the probability that Bush will win the election is 0.6 and the probability that he will lose is 0.8? The source of the problem is that we need to explain how subjective degrees of beliefs (something that is internal to each one of us) are reflected in our actions. This issue is a major concern in subjective probabilities. One possible way of attributing degrees of beliefs is by a betting game. Suppose you believe that P(Î±) = 0.8. Then you would be willing to place a bet of $1 against $3. To see this, note that with probability 0.8 you gain a dollar, and with probability 0.2 you lose $3, so on average this bet is a good deal with expected gain of 20 cents. In fact, you might be even tempted to place a bet of $1 against $4. Under this bet the average gain is 0, so you should not mind. However, you would not consider it worthwhile to place a bet $1 against $4 and 10 cents, since that would have negative expected gain. Thus, by finding which bets you are willing to place, we can assess your degrees of beliefs. The key point of this mental game is the following. If you hold degrees of belief that do not satisfy the rule of probability, then by a clever construction we can find a series of bets that would result in a sure negative outcome for you. Thus, the argument goes, a rational person must hold degrees of belief that satisfy the rules of probability.1 In the remainder of the book we discuss probabilities, but we usually do not explicitly state their interpretation. Since both interpretations lead to the same mathematical rules, the technical definitions hold for both interpretations.

2 Basic Concepts in Probability
2.1 Conditional Probability
To use a concrete example, suppose we consider a distribution over a population of students taking a certain course. The space of outcomes is simply the set of all students in the population. Now, suppose that we want to reason about the studentsâ€™ intelligence and their final grade. We can define the event Î± to denote â€œall students with grade A,â€ and the event Î² to denote â€œall students with high intelligence.â€ Using our distribution, we can consider the probability of these events, as well as the probability of Î± âˆ© Î² (the set of intelligent students who got grade A). This, however, does not directly tell us how to update our beliefs given new evidence. Suppose we learn that a student has received the grade A; what does that tell us about her intelligence? This kind of question arises every time we want to use distributions to reason about the real world. More precisely, after learning that an event Î± is true, how do we change our probability conditional about Î² occurring? The answer is via the notion of conditional probability. Formally, the probability conditional probability of Î² given Î± is defined as P(Î² | Î±) = P(Î± âˆ© Î²) P(Î±) (2.1) That is, the probability that Î² is true given that we know Î± is the relative proportion of outcomes satisfying Î² among these that satisfy Î±. (Note that the conditional probability is not defined when P(Î±) = 0.) The conditional probability given an event (say Î±) satisfies the properties of definition 2.1 (see exercise 2.4), and thus it is a probability distribution by its own right. Hence, we can think of the conditioning operation as taking one distribution and returning another over the same probability space.

2.2 Chain Rule and Bayes Rule
From the definition of the conditional distribution, we immediately see that P(Î± âˆ© Î²) = P(Î±)P(Î² | Î±). (2.2) chain rule This equality is known as the chain rule of conditional probabilities. More generally, if Î±1, . . . , Î±k are events, then we can write P(Î±1 âˆ© . . . âˆ© Î±k) = P(Î±1)P(Î±2 | Î±1) Â· Â· Â· P(Î±k | Î±1 âˆ© . . . âˆ© Î±kâˆ’1). (2.3) In other words, we can express the probability of a combination of several events in terms of the probability of the first, the probability of the second given the first, and so on. It is important to notice that we can expand this expression using any order of events â€” the result will remain the same. Bayesâ€™ rule Another immediate consequence of the definition of conditional probability is Bayesâ€™ rule P(Î± | Î²) = P(Î² | Î±)P(Î±) P(Î²)

A more general conditional version of Bayesâ€™ rule, where all our probabilities are conditioned on some background event Î³, also holds: P(Î± | Î² âˆ© Î³) = P(Î² | Î± âˆ© Î³)P(Î± | Î³) P(Î² | Î³) . Bayesâ€™ rule is important in that it allows us to compute the conditional probability P(Î± | Î²) from the â€œinverseâ€ conditional probability P(Î² | Î±). Example 2.1 Consider the student population, and let Smart denote smart students and GradeA denote students who got grade A. Assume we believe (perhaps based on estimates from past statistics) that P(GradeA | Smart) = 0.6, and now we learn that a particular student received grade A. Can we estimate the probability that the student is smart? According to Bayesâ€™ rule, this depends on prior our prior probability for students being smart (before we learn anything about them) and the prior probability of students receiving high grades. For example, suppose that P(Smart) = 0.3 and P(GradeA) = 0.2, then we have that P(Smart | GradeA) = 0.6 âˆ— 0.3/0.2 = 0.9. That is, an A grade strongly suggests that the student is smart. On the other hand, if the test was easier and high grades were more common, say, P(GradeA) = 0.4 then we would get that P(Smart | GradeA) = 0.6 âˆ— 0.3/0.4 = 0.45, which is much less conclusive about the student. Another classic example that shows the importance of this reasoning is in disease screening. To see this, consider the following hypothetical example (none of the mentioned figures are related to real statistics). Example 2.2 Suppose that a tuberculosis (TB) skin test is 95 percent accurate. That is, if the patient is TB-infected, then the test will be positive with probability 0.95, and if the patient is not infected, then the test will be negative with probability 0.95. Now suppose that a person gets a positive test result. What is the probability that he is infected? Naive reasoning suggests that if the test result is wrong 5 percent of the time, then the probability that the subject is infected is 0.95. That is, 95 percent of subjects with positive results have TB. If we consider the problem by applying Bayesâ€™ rule, we see that we need to consider the prior probability of TB infection, and the probability of getting positive test result. Suppose that 1 in 1000 of the subjects who get tested is infected. That is, P(TB) = 0.001. What is the probability of getting a positive test result? From our description, we see that 0.001 Â· 0.95 infected subjects get a positive result, and 0.999Â·0.05 uninfected subjects get a positive result. Thus, P(Positive) = 0.0509. Applying Bayesâ€™ rule, we get that P(TB | Positive) = 0.001Â·0.95/0.0509 â‰ˆ 0.0187. Thus, although a subject with a positive test is much more probable to be TB-infected than is a random subject, fewer than 2 percent of these subjects are TB-infected.

3 Random Variables and Joint Distributions
3.1 Motivation
Our discussion of probability distributions deals with events. Formally, we can consider any event from the set of measurable events. The description of events is in terms of sets of outcomes. In many cases, however, it would be more natural to consider attributes of the outcome. For example, if we consider a patient, we might consider attributes such as â€œage,â€

â€œgender,â€ and â€œsmoking historyâ€ that are relevant for assigning probability over possible diseases and symptoms. We would like then consider events such as â€œage > 55, heavy smoking history, and suers from repeated cough.â€ To use a concrete example, consider again a distribution over a population of students in a course. Suppose that we want to reason about the intelligence of students, their final grades, and so forth. We can use an event such as GradeA to denote the subset of students that received the grade A and use it in our formulation. However, this discussion becomes rather cumbersome if we also want to consider students with grade B, students with grade C, and so on. Instead, we would like to consider a way of directly referring to a studentâ€™s grade in a clean, mathematical way. The formal machinery for discussing attributes and their values in dierent outcomes are random variable random variables. A random variable is a way of reporting an attribute of the outcome. For example, suppose we have a random variable Grade that reports the final grade of a student, then the statement P (Grade = A) is another notation for P (GradeA).

n the statement P (Grade = A) is another notation for P (GradeA).

3.2 What Is a Random Variable?
Formally, a random variable, such as Grade, is defined by a function that associates with each outcome in â„¦ a value. For example, Grade is defined by a function fGrade that maps each person in â„¦ to his or her grade (say, one of A, B, or C). The event Grade = A is a shorthand for the event {Ï‰ âˆˆ â„¦ : fGrade(Ï‰) = A}. In our example, we might also have a random variable Intelligence that (for simplicity) takes as values either â€œhighâ€ or â€œlow.â€ In this case, the event â€œIntelligence = highâ€ refers, as can be expected, to the set of smart (high intelligence) students. Random variables can take dierent sets of values. We can think of categorical (or discrete) random variables that take one of a few values, as in our two preceding examples. We can also talk about random variables that can take infinitely many values (for example, integer or real values), such as Height that denotes a studentâ€™s height. We use Val(X) to denote the set of values that a random variable X can take. In most of the discussion in this book we examine either categorical random variables or random variables that take real values. We will usually use uppercase roman letters X, Y, Z to denote random variables. In discussing generic random variables, we often use a lowercase letter to refer to a value of a random variable. Thus, we use x to refer to a generic value of X. For example, in statements such as â€œP (X = x) â‰¥ 0 for all x âˆˆ Val(X).â€ When we discuss categorical random variables, we use the notation x1, . . . , xk, for k = |Val(X)| (the number of elements in Val(X)), when we need to enumerate the specific values of X, for example, in statements such as kX i =1 P (X = xi) = 1. multinomial The distribution over such a variable is called a multinomial. In the case of a binary-valued distribution random variable X, where Val(X) = {false, true}, we often use x1 to denote the value true for X, and x0 to denote the value false. The distribution of such a random variable is called a Bernoulli Bernoulli distribution. distribution We also use boldface type to denote sets of random variables. Thus, X, Y , or Z are typically used to denote a set of random variables, while x, y, z denote assignments of values to the

variables in these sets. We extend the definition of Val(X) to refer to sets of variables in the obvious way. Thus, x is always a member of Val(X). For Y âŠ† X, we use xhY i to refer to the assignment within x to the variables in Y . For two assignments x (to X) and y (to Y ), we say that x âˆ¼ y if they agree on the variables in their intersection, that is, xhX âˆ© Y i = yhX âˆ© Y i. In many cases, the notation P(X = x) is redundant, since the fact that x is a value of X is already reported by our choice of letter. Thus, in many texts on probability, the identity of a random variable is not explicitly mentioned, but can be inferred through the notation used for its value. Thus, we use P(x) as a shorthand for P(X = x) when the identity of the random variable is clear from the context. Another shorthand notation is that Px refers to a sum over all possible values that X can take. Thus, the preceding statement will often appear as Px P(x) = 1. Finally, another standard notation has to do with conjunction. Rather than write P((X = x) âˆ© (Y = y)), we write P(X = x, Y = y), or just P(x, y).

3.3 Marginal and Joint Distributions
Once we define a random variable X, we can consider the distribution over events that can be marginal described using X. This distribution is often referred to as the marginal distribution over the distribution random variable X. We denote this distribution by P(X). Returning to our population example, consider the random variable Intelligence. The marginal distribution over Intelligence assigns probability to specific events such as P(Intelligence = high) and P(Intelligence = low), as well as to the trivial event P(Intelligence âˆˆ {high, low}). Note that these probabilities are defined by the probability distribution over the original space. For concreteness, suppose that P(Intelligence = high) = 0.3, P(Intelligence = low) = 0.7. If we consider the random variable Grade, we can also define a marginal distribution. This is a distribution over all events that can be described in terms of the Grade variable. In our example, we have that P(Grade = A) = 0.25, P(Grade = B) = 0.37, and P(Grade = C) = 0.38. It should be fairly obvious that the marginal distribution is a probability distribution satisfying the properties of definition 2.1. In fact, the only change is that we restrict our attention to the subsets of S that can be described with the random variable X. In many situations, we are interested in questions that involve the values of several random variables. For example, we might be interested in the event â€œIntelligence = high and Grade = A.â€ joint distribution To discuss such events, we need to consider the joint distribution over these two random variables. In general, the joint distribution over a set X = {X1, . . . , Xn} of random variables is denoted by P(X1, . . . , Xn) and is the distribution that assigns probabilities to events that are specified in terms of these random variables. We use Î¾ to refer to a full assignment to the variables in X , that is, Î¾ âˆˆ Val(X). The joint distribution of two random variables has to be consistent with the marginal distribution, in that P(x) = P y P(x, y). This relationship is shown in figure 2.1, where we compute the marginal distribution over Grade by summing the probabilities along each row. Similarly, we find the marginal distribution over Intelligence by summing out along each column. The resulting sums are typically written in the row or column margins, whence the term â€œmarginal distribution.â€ Suppose we have a joint distribution over the variables X = {X1, . . . , Xn}. The most fine-grained events we can discuss using these variables are ones of the form â€œX1 = x1 and X2 = x2, . . ., and Xn = xnâ€ for a choice of values x1, . . . , xn for all the variables. Moreover,

Intelligence low high A 0.07 0.18 0.25 Grade B 0.28 0.09 0.37 C 0.35 0.03 0.38 0.7 0.3 1 Figure 2.1 Example of a joint distribution P(Intelligence, Grade): Values of Intelligence (columns) and Grade (rows) with the associated marginal distribution on each variable. any two such events must be either identical or disjoint, since they both assign values to all the variables in X . In addition, any event defined using variables in X must be a union of a set of canonical such events. Thus, we are eectively working in a canonical outcome space: a space where each outcome space outcome corresponds to a joint assignment to X1, . . . , Xn. More precisely, all our probability computations remain the same whether we consider the original outcome space (for example, all students), or the canonical space (for example, all combinations of intelligence and grade). atomic outcome We use Î¾ to denote these atomic outcomes: those assigning a value to each variable in X . For example, if we let X = {Intelligence, Grade}, there are six atomic outcomes, shown in figure 2.1. The figure also shows one possible joint distribution over these six outcomes. Based on this discussion, from now on we will not explicitly specify the set of outcomes and measurable events, and instead implicitly assume the canonical outcome space.

3.4 Conditional Probability
The notion of conditional probability extends to induced distributions over random variables. For conditional example, we use the notation P (Intelligence | Grade = A) to denote the conditional distribution distribution over the events describable by Intelligence given the knowledge that the studentâ€™s grade is A. Note that the conditional distribution over a random variable given an observation of the value of another one is not the same as the marginal distribution. In our example, P (Intelligence = high) = 0.3, and P (Intelligence = high | Grade = A) = 0.18/0.25 = 0.72. Thus, clearly P (Intelligence | Grade = A) is dierent from the marginal distribution P (Intelligence). The latter distribution represents our prior knowledge about students before learning anything else about a particular student, while the conditional distribution represents our more informed distribution after learning her grade. We will often use the notation P (X | Y ) to represent a set of conditional probability distributions. Intuitively, for each value of Y , this object assigns a probability over values of X using the conditional probability. This notation allows us to write the shorthand version of the chain rule: P (X, Y ) = P (X)P (Y | X), which can be extended to multiple variables as P (X1, . . . , Xk) = P (X1)P (X2 | X1) Â· Â· Â· P (Xk | X1, . . . , Xkâˆ’1). (2.5) Similarly, we can state Bayesâ€™ rule in terms of conditional probability distributions: P (X | Y ) = P (X)P (Y | X) P (Y ) . (2.6)

4 Independence and Conditional Independence
4.1 Independence
As we mentioned, we usually expect P(Î±|Î²)P(Î±|Î²) to be different from P(Î±)P(Î±). That is, learning that Î²Î² is true changes our probability over Î±Î±. However, in some situations equality can occur, so that P(Î±|Î²)=P(Î±)P(Î±|Î²)=P(Î±). That is, learning that Î²Î² occurs did not change our probability of Î±Î±.

Definition independent events

We say that an event Î±Î± is independent of event Î²Î² in PP, denoted PâŠ¨(Î±âŠ¥Î²)PâŠ¨(Î±âŠ¥Î²), if P(Î±|Î²)=P(Î±)P(Î±|Î²)=P(Î±) or if P(Î²)=0P(Î²)=0.

We can also provide an alternative definition for the concept of independence:

Proposition 2.1

A distribution PP satisfies (Î±âŠ¥Î²)(Î±âŠ¥Î²) if and only if P(Î±âˆ©Î²)=P(Î±)P(Î²)P(Î±âˆ©Î²)=P(Î±)P(Î²).

PROOF Consider first the case where P(Î²)=0P(Î²)=0; here, we also have P(Î±âˆ©Î²)=0P(Î±âˆ©Î²)=0, and so the equivalence immediately holds. When P(Î²)â‰ 0P(Î²)â‰ 0, we can use the chain rule; we write P(Î±âˆ©Î²)=P(Î±|Î²)P(Î²)P(Î±âˆ©Î²)=P(Î±|Î²)P(Î²). Since Î±Î± is independent of Î²Î², we have that P(Î±|Î²)=P(Î±)P(Î±|Î²)=P(Î±). Thus, P(Î±âˆ©Î²)=P(Î±)P(Î²)P(Î±âˆ©Î²)=P(Î±)P(Î²). Conversely, suppose that P(Î±âˆ©Î²)=P(Î±)P(Î²)P(Î±âˆ©Î²)=P(Î±)P(Î²). Then, by definition, we have that

P(Î±|Î²)=P(Î±âˆ©Î²)P(Î²)=P(Î±)P(Î²)P(Î²)=P(Î±).
P(Î±|Î²)=P(Î±âˆ©Î²)P(Î²)=P(Î±)P(Î²)P(Î²)=P(Î±).
As an immediate consequence of this alternative definition, we see that independence is a symmetric notion. That is, (Î± âŠ¥ Î²) implies (Î² âŠ¥ Î±). Example 2.3 For example, suppose that we toss two coins, and let Î± be the event â€œthe first toss results in a headâ€ and Î² the event â€œthe second toss results in a head.â€ It is not hard to convince ourselves that we expect that these two events to be independent. Learning that Î² is true would not change our probability of Î±. In this case, we see two dierent physical processes (that is, coin tosses) leading to the events, which makes it intuitive that the probabilities of the two are independent. In certain cases, the same process can lead to independent events. For example, consider the event Î± denoting â€œthe die outcome is evenâ€ and the event Î² denoting â€œthe die outcome is 1 or 2.â€ It is easy to check that if the die is fair (each of the six possible outcomes has probability 1 6 ), then these two events are independent.

4.2 Conditional Independence
 While independence is a useful property, it is not often that we encounter two indepen- dent events. A more common situation is when two events are independent given an additional event. For example, suppose we want to reason about the chance that our student is accepted to graduate studies at Stanford or MIT. Denote by Stanford the event â€œadmitted to Stanfordâ€ and by MIT the event â€œadmitted to MIT.â€ In most reasonable distributions, these two events are not independent. If we learn that a student was admitted to Stanford, then our estimate of her probability of being accepted at MIT is now higher, since it is a sign that she is a promising student

Now, suppose that both universities base their decisions only on the studentâ€™s grade point average (GPA), and we know that our student has a GPA of A. In this case, we might argue that learning that the student was admitted to Stanford should not change the probability that she will be admitted to MIT: Her GPA already tells us the information relevant to her chances of admission to MIT, and finding out about her admission to Stanford does not change that. Formally, the statement is P(MIT | Stanford, GradeA) = P(MIT | GradeA). In this case, we say that MIT is conditionally independent of Stanford given GradeA. Definition 2.3 We say that an event Î± is conditionally independent of event Î² given event Î³ in P, denoted conditional independence P |= (Î± âŠ¥ Î² | Î³), if P(Î± | Î² âˆ© Î³) = P(Î± | Î³) or if P(Î² âˆ© Î³) = 0. It is easy to extend the arguments we have seen in the case of (unconditional) independencies to give an alternative definition. Proposition 2.2 P satisfies (Î± âŠ¥ Î² | Î³) if and only if P(Î± âˆ© Î² | Î³) = P(Î± | Î³)P(Î² | Î³).

4.3 Independence of Random Variables
Until now, we have focused on independence between events. Thus, we can say that two events, such as one toss landing heads and a second also landing heads, are independent. However, we would like to say that any pair of outcomes of the coin tosses is independent. To capture such statements, we can examine the generalization of independence to sets of random variables. Definition 2.4 Let X, Y , Z be sets of random variables. We say that X is conditionally independent of Y given conditional independence Z in a distribution P if P satisfies (X = x âŠ¥ Y = y | Z = z) for all values x âˆˆ Val(X), y âˆˆ Val(Y ), and z âˆˆ Val(Z). The variables in the set Z are often said to be observed. If the set observed variable Z is empty, then instead of writing (X âŠ¥ Y | âˆ…), we write (X âŠ¥ Y ) and say that X and Y are marginally independent. marginal independence Thus, an independence statement over random variables is a universal quantification over all possible values of the random variables. The alternative characterization of conditional independence follows immediately: Proposition 2.3 The distribution P satisfies (X âŠ¥ Y | Z) if and only if P(X, Y | Z) = P(X | Z)P(Y | Z). Suppose we learn about a conditional independence. Can we conclude other independence properties that must hold in the distribution? We have already seen one such example: symmetry â€¢ Symmetry: (X âŠ¥ Y | Z) =â‡’ (Y âŠ¥ X | Z). (2.7) There are several other properties that hold for conditional independence, and that often provide a very clean method for proving important properties about distributions. Some key properties are:

â€¢ Decomposition: (X âŠ¥ Y , W | Z) =â‡’ (X âŠ¥ Y | Z). (2.8) weak union â€¢ Weak union: (X âŠ¥ Y , W | Z) =â‡’ (X âŠ¥ Y | Z, W). (2.9) contraction â€¢ Contraction: (X âŠ¥ W | Z, Y )&(X âŠ¥ Y | Z) =â‡’ (X âŠ¥ Y , W | Z). (2.10) An additional important property does not hold in general, but it does hold in an important subclass of distributions. Definition 2.5 A distribution P is said to be positive if for all events Î± âˆˆ S such that Î± 6= âˆ…, we have that positive distribution P(Î±) > 0. For positive distributions, we also have the following property: intersection â€¢ Intersection: For positive distributions, and for mutually disjoint sets X, Y , Z, W : (X âŠ¥ Y | Z, W)&(X âŠ¥ W | Z, Y ) =â‡’ (X âŠ¥ Y , W | Z). (2.11) The proof of these properties is not dicult. For example, to prove Decomposition, assume that (X âŠ¥ Y, W | Z) holds. Then, from the definition of conditional independence, we have that P(X, Y, W | Z) = P(X | Z)P(Y, W | Z). Now, using basic rules of probability and arithmetic, we can show P(X, Y | Z) = X w P(X, Y, w | Z) = X w P(X | Z)P(Y, w | Z) = P(X | Z) X w P(Y, w | Z) = P(X | Z)P(Y | Z). The only property we used here is called â€œreasoning by casesâ€ (see exercise 2.6). We conclude that (X âŠ¥ Y | Z).

5 Querying a Distribution
Our focus throughout this book is on using a joint probability distribution over multiple random variables to answer queries of interest.

5.1 Probability Queries
probability query Perhaps the most common query type is the probability query. Such a query consists of two parts: evidence â€¢ The evidence: a subset E of random variables in the model, and an instantiation e to these variables; query variables â€¢ the query variables: a subset Y of random variables in the network. Our task is to compute P(Y | E = e), posterior that is, the posterior probability distribution over the values y of Y , conditioned on the fact that distribution E = e. This expression can also be viewed as the marginal over Y , in the distribution we obtain by conditioning on e.

5.2 MAP Queries
A second important type of task is that of finding a high-probability joint assignment to some subset of variables. The simplest variant of this type of task is the MAP query (also called MAP assignment most probable explanation (MPE)), whose aim is to find the MAP assignment â€” the most likely assignment to all of the (non-evidence) variables. More precisely, if we let W = X âˆ’ E, our task is to find the most likely assignment to the variables in W given the evidence E = e: MAP(W | e) = argmax w P(w, e), (2.12) where, in general, argmaxx f(x) represents the value of x for which f(x) is maximal. Note that there might be more than one assignment that has the highest posterior probability. In this case, we can either decide that the MAP task is to return the set of possible assignments, or to return an arbitrary member of that set. It is important to understand the dierence between MAP queries and probability queries. In a MAP query, we are finding the most likely joint assignment to W . To find the most likely assignment to a single variable A, we could simply compute P(A | e) and then pick the most likely value. However, the assignment where each variable individually picks its most  likely value can be quite dierent from the most likely joint assignment to all variables simultaneously. This phenomenon can occur even in the simplest case, where we have no evidence. Example 2.4 Consider a two node chain A â†’ B where A and B are both binary-valued. Assume that: a0 a1 0.4 0.6 A b0 b1 a0 0.1 0.9 a1 0.5 0.5 (2.13) We can see that P(a1) > P(a0), so that MAP(A) = a1. However, MAP(A, B) = (a0, b1): Both values of B have the same probability given a1. Thus, the most likely assignment containing a1 has probability 0.6 Ã— 0.5 = 0.3. On the other hand, the distribution over values of B is more skewed given a0, and the most likely assignment (a0, b1) has the probability 0.4 Ã— 0.9 = 0.36. Thus, we have that argmaxa,b P(a, b) 6= (argmaxa P(a),argmaxb P(b)).

5.3 Marginal MAP Queries
To motivate our second query type, let us return to the phenomenon demonstrated in example 2.4. Now, consider a medical diagnosis problem, where the most likely disease has multiple possible symptoms, each of which occurs with some probability, but not an overwhelming probability. On the other hand, a somewhat rarer disease might have only a few symptoms, each of which is very likely given the disease. As in our simple example, the MAP assignment to the data and the symptoms might be higher for the second disease than for the first one. The solution here is to look for the most likely assignment to the disease variable(s) only, rather than the most likely assignment to both the disease and symptom variables. This approach suggests marginal MAP the use of a more general query type. In the marginal MAP query, we have a subset of variables Y that forms our query. The task is to find the most likely assignment to the variables in Y given the evidence E = e: MAP(Y | e) = arg max y P(y | e). If we let Z = X âˆ’ Y âˆ’ E, the marginal MAP task is to compute: MAP(Y | e) = arg max Y X Z P(Y , Z | e). Thus, marginal MAP queries contain both summations and maximizations; in a way, it contains elements of both a conditional probability query and a MAP query. Note that example 2.4 shows that marginal MAP assignments are not monotonic: the most likely assignment MAP(Y1 | e) might be completely dierent from the assignment to Y1 in MAP({Y1, Y2} | e). Thus, in particular, we cannot use a MAP query to give us the correct answer to a marginal MAP query.

6 Continuous Spaces
In the previous section, we focused on random variables that have a finite set of possible values. In many situations, we also want to reason about continuous quantities such as weight, height, duration, or cost that take real numbers in IR. When dealing with probabilities over continuous random variables, we have to deal with some technical issues. For example, suppose that we want to reason about a random variable X that can take values in the range between 0 and 1. That is, Val(X) is the interval [0, 1]. Moreover, assume that we want to assign each number in this range equal probability. What would be the probability of a number x? Clearly, since each x has the same probability, and there are infinite number of values, we must have that P(X = x) = 0. This problem appears even if we do not require uniform probability.

6.1 Probability Density Functions
How do we define probability over a continuous random variable? We say that a function density function p : IR 7â†’ IR is a probability density function or (PDF) for X if it is a nonnegative integrable

function such that Z Val(X) p(x)dx = 1. That is, the integral over the set of possible values of X is 1. The PDF defines a distribution for X as follows: for any x in our event space: P(X â‰¤ a) = aZ âˆ’âˆž p(x)dx. cumulative The function P is the cumulative distribution for X. We can easily employ the rules of distribution probability to see that by using the density function we can evaluate the probability of other events. For example, P(a â‰¤ X â‰¤ b) = bZa p(x)dx. Intuitively, the value of a PDF p(x) at a point x is the incremental amount that x adds to the cumulative distribution in the integration process. The higher the value of p at and around x, the more mass is added to the cumulative distribution as it passes x. The simplest PDF is the uniform distribution. Definition 2.6 A variable X has a uniform distribution over [a, b], denoted X âˆ¼ Unif[a,b] if it has the PDF uniform distribution p(x) =  0 otherwise bâˆ’ 1a b â‰¥ x â‰¥ a. Thus, the probability of any subinterval of [a, b] is proportional its size relative to the size of [a, b]. Note that, if b âˆ’ a < 1, then the density can be greater than 1. Although this looks unintuitive, this situation can occur even in a legal PDF, if the interval over which the value is greater than 1 is not too large. We have only to satisfy the constraint that the total area under the PDF is 1. As a more complex example, consider the Gaussian distribution. Definition 2.7 A random variable X has a Gaussian distribution with mean Âµ and variance Ïƒ2, denoted X âˆ¼ Gaussian distribution N Âµ; Ïƒ2, if it has the PDF p(x) = âˆš21Ï€Ïƒ eâˆ’ (x2 âˆ’ ÏƒÂµ 2)2 . standard A standard Gaussian is one with mean 0 and variance 1. Gaussian A Gaussian distribution has a bell-like curve, where the mean parameter Âµ controls the location of the peak, that is, the value for which the Gaussian gets its maximum value. The variance parameter Ïƒ2 determines how peaked the Gaussian is: the smaller the variance, the


more peaked the Gaussian. Figure 2.2 shows the probability density function of a few dierent Gaussian distributions. More technically, the probability density function is specified as an exponential, where the expression in the exponent corresponds to the square of the number of standard deviations Ïƒ that x is away from the mean Âµ. The probability of x decreases exponentially with the square of its deviation from the mean, as measured in units of its standard deviation.

6.2 Joint Density Functions
The discussion of density functions for a single variable naturally extends for joint distributions of continuous random variables. Definition 2.8 Let P be a joint distribution over continuous random variables X1, . . . , Xn. A function p(x1, . . . , xn) joint density is a joint density function of X1, . . . , Xn if â€¢ p(x1, . . . , xn) â‰¥ 0 for all values x1, . . . , xn of X1, . . . , Xn. â€¢ p is an integrable function. â€¢ For any choice of a1, . . . , an, and b1, . . . , bn, P(a1 â‰¤ X1 â‰¤ b1, . . . , an â‰¤ Xn â‰¤ bn) = b1 Za1 Â· Â· Â· b nZ a n p(x1, . . . , xn)dx1 . . . dxn. Thus, a joint density specifies the probability of any joint event over the variables of interest. Both the uniform distribution and the Gaussian distribution have natural extensions to the multivariate case. The definition of a multivariate uniform distribution is straightforward. We defer the definition of the multivariate Gaussian to section 7.1. From the joint density we can derive the marginal density of any random variable by integrating out the other variables. Thus, for example, if p(x, y) is the joint density of X and Y

then p(x) = âˆž Z âˆ’âˆž p(x, y)dy. To see why this equality holds, note that the event a â‰¤ X â‰¤ b is, by definition, equal to the event â€œa â‰¤ X â‰¤ b and âˆ’âˆž â‰¤ Y â‰¤ âˆž.â€ This rule is the direct analogue of marginalization for discrete variables. Note that, as with discrete probability distributions, we abuse notation a bit and use p to denote both the joint density of X and Y and the marginal density of X. In cases where the distinction is not clear, we use subscripts, so that pX will be the marginal density, of X, and pX,Y the joint density.

6.3 Conditional Density Functions
As with discrete random variables, we want to be able to describe conditional distributions of continuous variables. Suppose, for example, we want to define P(Y | X = x). Applying the definition of conditional distribution (equation (2.1)), we run into a problem, since P(X = x) = 0. Thus, the ratio of P(Y, X = x) and P(X = x) is undefined. To avoid this problem, we might consider conditioning on the event x âˆ’  â‰¤ X â‰¤ x + , which can have a positive probability. Now, the conditional probability is well defined. Thus, we might consider the limit of this quantity when  â†’ 0. We define P(Y | x) = lim â†’0 P(Y | x âˆ’  â‰¤ X â‰¤ x + ). When does this limit exist? If there is a continuous joint density function p(x, y), then we can derive the form for this term. To do so, consider some event on Y , say a â‰¤ Y â‰¤ b. Recall that P(a â‰¤ Y â‰¤ b | x âˆ’  â‰¤ X â‰¤ x + ) = P(a â‰¤ Y â‰¤ b, x âˆ’  â‰¤ X â‰¤ x + ) P(x âˆ’  â‰¤ X â‰¤ x + ) = Ra b Rx xâˆ’ + p(x0, y)dydx0 Rx xâˆ’ + p(x0)dx0 . When  is suciently small, we can approximate x+ Z xâˆ’ p(x0)dx0 â‰ˆ 2p(x). Using a similar approximation for p(x0, y), we get P(a â‰¤ Y â‰¤ b | x âˆ’  â‰¤ X â‰¤ x + ) â‰ˆ Ra b 2p(x, y)dy 2p(x) = bZa p(x, y) p(x) dy. We conclude that p(x,y) p(x) is the density of P(Y | X = x).

Let p(x, y) be the joint density of X and Y . The conditional density function of Y given X is conditional density function defined as p(y | x) = p(x, y) p(x) When p(x) = 0, the conditional density is undefined. The conditional density p(y | x) characterizes the conditional distribution P(Y | X = x) we defined earlier. The properties of joint distributions and conditional distributions carry over to joint and conditional density functions. In particular, we have the chain rule p(x, y) = p(x)p(y | x) (2.14) and Bayesâ€™ rule p(x | y) = p(x)p(y | x) p(y) . (2.15) As a general statement, whenever we discuss joint distributions of continuous random variables, we discuss properties with respect to the joint density function instead of the joint distribution, as we do in the case of discrete variables. Of particular interest is the notion of (conditional) independence of continuous random variables. Definition 2.10 Let X, Y , and Z be sets of continuous random variables with joint density p(X, Y , Z). We say conditional that X is conditionally independent of Y given Z if independence p(x | z) = p(x | y, z) for all x, y, z such that p(z) > 0.

7 Expectation and Variance
7.1 Expectation
expectation Let X be a discrete random variable that takes numerical values; then the expectation of X under the distribution P is IEP[X] = X x x Â· P(x). If X is a continuous variable, then we use the density function IEP[X] = Z x Â· p(x)dx. For example, if we consider X to be the outcome of rolling a fair die with probability 1/6 for each outcome, then IE[X] = 1 Â· 1 6 + 2 Â· 1 6 + Â· Â· Â· + 6 Â· 1 6 = 3.5. On the other hand, if we consider a biased die where P(X = 6) = 0.5 and P(X = x) = 0.1 for x < 6, then IE[X] = 1 Â· 0.1 + Â· Â· Â· + 5 Â· 0.1 + Â· Â· Â· + 6 Â· 0.5 = 4.5.

Often we are interested in expectations of a function of a random variable (or several random variables). Thus, we might consider extending the definition to consider the expectation of a functional term such as X2 + 0.5X. Note, however, that any function g of a set of random variables X1, . . . , Xk is essentially defining a new random variable Y : For any outcome Ï‰ âˆˆ â„¦, we define the value of Y as g(fX1(Ï‰), . . . , fXk(Ï‰)). Based on this discussion, we often define new random variables by a functional term. For example Y = X2, or Y = eX. We can also consider functions that map values of one or more categorical random variables to numerical values. One such function that we use quite often is indicator function the indicator function, which we denote 11{X = x}. This function takes value 1 when X = x, and 0 otherwise. In addition, we often consider expectations of functions of random variables without bothering to name the random variables they define. For example IEP [X + Y ]. Nonetheless, we should keep in mind that such a term does refer to an expectation of a random variable. We now turn to examine properties of the expectation of a random variable. First, as can be easily seen, the expectation of a random variable is a linear function in that random variable. Thus, IE[a Â· X + b] = aIE[X] + b. A more complex situation is when we consider the expectation of a function of several random variables that have some joint behavior. An important property of expectation is that the expectation of a sum of two random variables is the sum of the expectations. Proposition 2.4 IE[X + Y ] = IE[X] + IE[Y ]. linearity of This property is called linearity of expectation. It is important to stress that this identity is true expectation even when the variables are not independent. As we will see, this property is key in simplifying many seemingly complex problems. Finally, what can we say about the expectation of a product of two random variables? In general, very little: Example 2.5 Consider two random variables X and Y , each of which takes the value +1 with probability 1/2, and the value âˆ’1 with probability 1/2. If X and Y are independent, then IE[X Â· Y ] = 0. On the other hand, if X and Y are correlated in that they always take the same value, then IE[X Â· Y ] = 1. However, when X and Y are independent, then, as in our example, we can compute the expectation simply as a product of their individual expectations: Proposition 2.5 If X and Y are independent, then IE[X Â· Y ] = IE[X] Â· IE[Y ]. conditional We often also use the expectation given some evidence. The conditional expectation of X expectation given y is IEP [X | y] = X x x Â· P(x | y).

7.2 Variance
The expectation of X tells us the mean value of X. However, It does not indicate how far X variance deviates from this value. A measure of this deviation is the variance of X. VVarP [X] = IEP h(X âˆ’ IEP [X])2i. Thus, the variance is the expectation of the squared dierence between X and its expected value. It gives us an indication of the spread of values of X around the expected value. An alternative formulation of the variance is VVar[X] = IEX2 âˆ’ (IE[X])2 . (2.16) (see exercise 2.11). Similar to the expectation, we can consider the expectation of a functions of random variables. Proposition 2.6 If X and Y are independent, then VVar[X + Y ] = VVar[X] + VVar[Y ]. It is straightforward to show that the variance scales as a quadratic function of X. In particular, we have: VVar[a Â· X + b] = a2VVar[X]. For this reason, we are often interested in the square root of the variance, which is called the standard standard deviation of the random variable. We define deviation ÏƒX = pVVar[X]. The intuition is that it is improbable to encounter values of X that are farther than several standard deviations from the expected value of X. Thus, ÏƒX is a normalized measure of â€œdistanceâ€ from the expected value of X. As an example consider the Gaussian distribution of definition 2.7. Proposition 2.7 Let X be a random variable with Gaussian distribution N(Âµ, Ïƒ2), then IE[X] = Âµ and VVar[X] = Ïƒ2. Thus, the parameters of the Gaussian distribution specify the expectation and the variance of the distribution. As we can see from the form of the distribution, the density of values of X drops exponentially fast in the distance xâˆ’Âµ Ïƒ . Not all distributions show such a rapid decline in the probability of outcomes that are distant from the expectation. However, even for arbitrary distributions, one can show that there is a decline. Theorem 2.1 (Chebyshev inequality): Chebyshevâ€™s inequality P (|X âˆ’ IEP [X]| â‰¥ t) â‰¤ VVarP [X] t2 .

We can restate this inequality in terms of standard deviations: We write t = kÏƒX to get P(|X âˆ’ IEP [X]| â‰¥ kÏƒX) â‰¤ 1 k2. Thus, for example, the probability of X being more than two standard deviations away from IE[X] is less than 1/4.

\section{Foundation: Graph}

Perhaps the most pervasive concept in this book is the representation of a probability distribution using a graph as a data structure. In this section, we survey some of the basic concepts in graph theory used in the book.

1 Nodes and Edges
A graph is a data structure K consisting of a set of nodes and a set of edges. Throughout most this book, we will assume that the set of nodes is X = {X1,...,Xn}. A pair of nodes Xi,Xj directed edge can be connected by a directed edge Xi â†’ Xj or an undirected edge Xiâ€”Xj. Thus, the set undirected edge of edges E is a set of pairs, where each pair is one of Xi â†’ Xj, Xj â†’ Xi, or Xiâ€”Xj, for Xi,Xj âˆˆ X , i < j. We assume throughout the book that, for each pair of nodes Xi,Xj, at most one type of edge exists; thus, we cannot have both Xi â†’ Xj and Xj â†’ Xi, nor can we have Xi â†’ Xj and Xiâ€”Xj.2 The notation Xi â† Xj is equivalent to Xj â†’ Xi, and the notation X jâ€”Xi is equivalent to Xiâ€”Xj. We use Xi Xj to represent the case where Xi and X j are connected via some edge, whether directed (in any direction) or undirected. In many cases, we want to restrict attention to graphs that contain only edges of one kind directed graph or another. We say that a graph is directed if all edges are either Xi â†’ Xj or Xj â†’ Xi. We usually denote directed graphs as G. We say that a graph is undirected if all edges are Xiâ€”Xj. undirected graph We denote undirected graphs as H. We sometimes convert a general graph to an undirected graph by ignoring the directions on the edges. Definition 2.11 Given a graph K = (X, E), its undirected version is a graph H = (X, E0) where E0 = {Xâ€”Y : graphâ€™s undirected version X Y âˆˆ E}. Whenever we have that Xi â†’ Xj âˆˆ E, we say that Xj is the child of Xi in K, and that child Xi is the parent of Xj in K. When we have Xiâ€”Xj âˆˆ E, we say that Xi is a neighbor of parent neighbor Xj in K (and vice versa). We say that X and Y are adjacent whenever X Y âˆˆ E. We use PaX to denote the parents of X, ChX to denote its children, and NbX to denote its neighbors. We define the boundary of X, denoted BoundaryX, to be PaX âˆª NbX; for DAGs, this set is boundary simply Xâ€™s parents, and for undirected graphs Xâ€™s neighbors.3 Figure 2.3 shows an example of a graph K. There, we have that A is the only parent of C, and F,I are the children of C. The degree only neighbor of C is D, but its adjacent nodes are A,D,F,I. The degree of a node X is the number of edges in which it participates. Its indegree is the number of directed edges Y â†’ X. indegree The degree of a graph is the maximal degree of a node in the graph. 2. Note that our definition is somewhat restricted, in that it disallows cycles of length two, where Xi â†’ Xj â†’ Xi, and allows self-loops where Xi â†’ Xi. 3. When the graph is not clear from context, we often add the graph as an additional argument.


2 Subgraphs
In many cases, we want to consider only the part of the graph that is associated with a particular subset of the nodes. Definition 2.12 Let K = (X , E), and let X âŠ‚ X . We define the induced subgraph K[X] to be the graph (X, E0) induced subgraph where E0 are all the edges X Y âˆˆ E such that X, Y âˆˆ X. For example, figure 2.4a shows the induced subgraph K[C, D, I]. A type of subgraph that is often of particular interest is one that contains all possible edges. Definition 2.13 A subgraph over X is complete if every two nodes in X are connected by some edge. The set X complete subgraph is often called a clique; we say that a clique X is maximal if for any superset of nodes Y âŠƒ X, clique Y is not a clique. Although the subset of nodes X can be arbitrary, we are often interested in sets of nodes that preserve certain aspects of the graph structure. Definition 2.14 We say that a subset of nodes X âˆˆ X is upwardly closed in K if, for any X âˆˆ X, we have that upward closure BoundaryX âŠ‚ X. We define the upward closure of X to be the minimal upwardly closed subset

Y that contains X. We define the upwardly closed subgraph of X, denoted K+[X], to be the induced subgraph over Y , K[Y ]. For example, the set A, B, C, D, E is the upward closure of the set {C} in K. The upwardly closed subgraph of {C} is shown in figure 2.4b. The upwardly closed subgraph of {C, D, I} is shown in figure 2.4c.

3 Paths and Trails
Using the basic notion of edges, we can define dierent types of longer-range connections in the graph.

Definition path

We say that X1,...,XkX1,...,Xk form a path in the graph K=(X,E)K=(X,E) if, for every i=1,...,kâˆ’1i=1,...,kâˆ’1, we have that either Xiâ†’Xi+1Xiâ†’Xi+1 or Xiâˆ’Xi+1Xiâˆ’Xi+1. A path is directed if, for at least one i, we have Xiâ†’Xi+1Xiâ†’Xi+1.

Definition trail

We say that X1,...,XkX1,...,Xk form a trail in the graph K=(X,E)K=(X,E) if, for every i=1,...,kâˆ’1i=1,...,kâˆ’1, we have that Xiâ‡ŒXi+1Xiâ‡ŒXi+1.


In the graph KK of figure 2.3, A,C,D,E,IA,C,D,E,I is a path, and hence also a trail. On the other hand, A,C,F,G,DA,C,F,G,D is a trail, which is not a path.

Definition connected graph

A graph is connected if for every Xi,XjXi,Xj there is a trail between XiXi and XjXj.

We can now define longer-range relationships in the graph.

Definition ancestor, descendant

We say that XX is an ancestor of YY in K=(X,E)K=(X,E), and that YY is a descendant of XX, if there exists a directed path X1,...,XkX1,...,Xk with X1=XX1=X and Xk=YXk=Y. We use DescendantsXDescendantsX to denote Xâ€™s descendants, AncestorsXAncestorsX to denote Xâ€™s ancestors, and NonDescendantsXNonDescendantsX to denote the set of nodes in Xâˆ’DescendantsXXâˆ’DescendantsX.

In our example graph K, we have that F,G,IF,G,I are descendants of CC. The ancestors of CC are AA, via the path A,C,A,C, and BB, via the path B,E,D,CB,E,D,C.

A final useful notion is that of an ordering of the nodes in a directed graph that is consistent with the directionality its edges.

Definition topological ordering

Let G=(X,E)G=(X,E) be a graph. An ordering of the nodes X1,...,XnX1,...,Xn is a topological ordering relative to KK if, whenever we have Xiâ†’XjâˆˆEXiâ†’XjâˆˆE, then i<ji<j.

Appendix A.3.1 presents an algorithm for finding such a topological ordering.

4 Cycles and Loops
Note that, in general, we can have a cyclic path that leads from a node to itself, making that node its own descendant.

Definition 2.20 A cycle in K is a directed path X1, . . . , Xk where X1 = Xk. A graph is acyclic if it contains no cycle acyclic cycles. For most of this book, we will restrict attention to graphs that do not allow such cycles, since it is quite dicult to define a coherent probabilistic model over graphs with directed cycles. DAG A directed acyclic graph (DAG) is one of the central concepts in this book, as DAGs are the basic graphical representation that underlies Bayesian networks. For some of this book, we also use acyclic graphs that are partially directed. The graph K of figure 2.3 is acyclic. However, if we add the undirected edge Aâ€”E to K, we have a path A, C, D, E, A from A to itself. Clearly, adding a directed edge E â†’ A would also lead to a cycle. Note that prohibiting cycles does not imply that there is no trail from a node to itself. For example, K contains several trails: C, D, E, I, C as well as C, D, G, F, C. An acyclic graph containing both directed and undirected edges is called a partially directed PDAG acyclic graph or PDAG. The acyclicity requirement on a PDAG implies that the graph can be chain component decomposed into a directed graph of chain components, where the nodes within each chain component are connected to each other only with undirected edges. The acyclicity of a PDAG guarantees us that we can order the components so that all edges point from lower-numbered components to higher-numbered ones. Definition 2.21 Let K be a PDAG over X . Let K1, . . . , K` be a disjoint partition of X such that: â€¢ the induced subgraph over Ki contains no directed edges; â€¢ for any pair of nodes X âˆˆ Ki and Y âˆˆ Kj for i < j, an edge between X and Y can only be a directed edge X â†’ Y . chain component Each component Ki is called a chain component. chain graph Because of its chain structure, a PDAG is also called a chain graph. Example 2.6 In the PDAG of figure 2.3, we have six chain components: {A}, {B}, {C, D, E}, {F, G}, {H}, and {I}. This ordering of the chain components is one of several possible legal orderings. Note that when the PDAG is an undirected graph, the entire graph forms a single chain component. Conversely, when the PDAG is a directed graph (and therefore acyclic), each node in the graph is its own chain component.


Dierent from a cycle is the notion of a loop: Definition 2.22 A loop in K is a trail X1, . . . , Xk where X1 = Xk. A graph is singly connected if it contains loop singly connected no loops. A node in a singly connected graph is called a leaf if it has exactly one adjacent node. leaf A singly connected directed graph is also called a polytree. A singly connected undirected graph is polytree called a forest; if it is also connected, it is called a tree. forest tree We can also define a notion of a forest, or of a tree, for directed graphs. Definition 2.23 A directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is also connected. Note that polytrees are very dierent from trees. For example, figure 2.5 shows a graph that is a polytree but is not a tree, because several nodes have more than one parent. As we will discuss later in the book, loops in the graph increase the computational cost of various tasks. We conclude this section with a final definition relating to loops in the graph. This definition will play an important role in evaluating the cost of reasoning using graph-based representations. Definition 2.24 Let X1â€”X2â€” Â· Â· Â· â€”Xkâ€”X1 be a loop in the graph; a chord in the loop is an edge connecting chordal graph Xi and Xj for two nonconsecutive nodes Xi, Xj. An undirected graph H is said to be chordal if any loop X1â€”X2â€” Â· Â· Â· â€”Xkâ€”X1 for k â‰¥ 4 has a chord. Thus, for example, a loop Aâ€”Bâ€”Câ€”Dâ€”A (as in figure 1.1b) is nonchordal, but adding an edge Aâ€”C would render it chordal. In other words, in a chordal graph, the longest â€œminimal loopâ€ (one that has no shortcut) is a triangle. Thus, chordal graphs are often also called triangulated triangulated. graph We can extend the notion of chordal graphs to graphs that contain directed edges. Definition 2.25 A graph K is said to be chordal if its underlying undirected graph is chordal.

\section{Bayesian Network}

A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.

A Non-Causal Bayesian Network Example
Figure 1 shows a simple Bayesian network, which consists of only two nodes and one link. It represents the JPD of the variables Eye Color and Hair Color in a population of students (Snee, 1974). In this case, the conditional probabilities of Hair Color given the values of its parent node, Eye Color, are provided in a CPT. It is important to point out that this Bayesian network does not contain any causal assumptions, i.e. we have no knowledge of the causal order between the variables. Thus, the interpretation of this network should be merely statistical (informational).



A Causal Network Example
Figure 2 illustrates another simple yet typical Bayesian network. In contrast to the statistical relationships in Figure 1, the diagram in Figure 2 describes the causal relationships among the seasons of the year (X1X1), whether it is raining (X2X2), whether the sprinkler is on (X3X3), whether the pavement is wet (X4X4), and whether the pavement is slippery (X5X5). Here, the absence of a direct link between X1X1 and X5X5, for example, captures our understanding that there is no direct influence of season on slipperiness. The influence is mediated by the wetness of the pavement (if freezing were a possibility, a direct link could be added).



A Dynamic Bayesian Network Example
Entities that live in a changing environment must keep track of variables whose values change over time. Dynamic Bayesian networks capture this process by representing multiple copies of the state variables, one for each time step. A set of variables Xtâˆ’1Xtâˆ’1 and XtXt denotes the world state at times t-1 and t respectively. A set of evidence variables Et denotes the observations available at time t. The sensor model P(Et|Xt)P(Et|Xt) is encoded in the conditional probability distributions for the observable variables, given the state variables. The transition model P(Xt|Xtâˆ’1)P(Xt|Xtâˆ’1) relates the state at time t-1 to the state at time t. Keeping track of the world means computing the current probability distribution over world states given all past observations, i.e. P(Xt|E1,â€¦,Et)P(Xt|E1,â€¦,Et).

Dynamic Bayesian networks (DBN) are a generalization of Hidden Markov Models (HMM) and Kalman Filters (KF). Every HMM and KF can be represented with a DBN. Furthermore, the DBN representation of an HMM is much more compact and, thus, much better understandable. The nodes in the HMM represent the states of the system, whereas the nodes in the DBN represent the dimensions of the system. For example, the HMM representation of the valve system in Figure 2.3 is made of 26 nodes and 36 arcs, versus 9 nodes and 11 arcs in the DBN (Weber and Jouffe, 2003).

\section{Template Models for Bayesian Networks}

In many cases, we need to model distributions that have a recurring structure. In this module, we describe representations for two such situations. One is temporal scenarios, where we want to model a probabilistic structure that holds constant over time; here, we use Hidden Markov Models, or, more generally, Dynamic Bayesian Networks. The other is aimed at scenarios that involve multiple similar entities, each of whose properties is governed by a similar model; here, we use Plate Models.

Temporal Models
Our focus in this section is on modeling dynamic settings, where we are interested in reasoning about the state of the world as it evolves over time. We can model such settings in terms of a system state system state, whose value at time t is a snapshot of the relevant attributes (hidden or observed) of the system at time t. We assume that the system state is represented, as usual, as an assignment of values to some set of random variables X . We use X (t) i to represent the instantiation of the variable Xi at time t. Note that Xi itself is no longer a variable that takes a value; rather, it is a template variable template variable. This template is instantiated at dierent points in time t, and each Xi (t) is a variable that takes a value in Val(Xi). For a set of variables X âŠ† X , we use X (t1:t2) (t1 < t2) to denote the set of variables {X (t) : t âˆˆ [t1,t2]}. As usual, we use the notation x(t:t0) for an assignment of values to this set of variables.

Each â€œpossible worldâ€ in our probability space is now a trajectory: an assignment of values to each variable X (t) i for each relevant time t. Our goal therefore is to represent a joint distribution over such trajectories. Clearly, the space of possible trajectories is a very complex probability space, so representing such a distribution can be very dicult. We therefore make a series of simplifying assumptions that help make this representational problem more tractable.

Dynamic Bayesian Networks




Directed Probabilistic Models for Object-Relational Domains
Based on the framework described in the previous section, we now describe template-based representation languages that can encode directed probabilistic models.

Plate Models
We begin our discussion by presenting the plate model, the simplest and best-established of the object-relational frameworks. Although restricted in several important ways, the plate modeling framework is perhaps the approach that has been most commonly used in practice, notably for encoding the assumptions made in various learning tasks. This framework also provides an excellent starting point for describing the key ideas of template-based languages and for motivating some of the extensions that have been pursued in richer languages.

In the plate formalism, object types are called plates. The fact that multiple objects in the class share the same set of attributes and same probabilistic model is the basis for the use of the term â€œplate,â€ which suggests a stack of identical objects. We begin with some motivating examples and then describe the formal framework.

Examples
Example 1 The simplest example of a plate model, shown in figure 6.6, describes multiple random variables generated from the same distribution. In this case, we have a set of random variables X(d) (dâˆˆD)X(d) (dâˆˆD) that all have the same domain Val(X) and are sampled from the same distribution. In a plate representation, we encode the fact that these variables are all generated from the same template by drawing only a single node X(d) and enclosing it in a box denoting that d ranges over D, so that we know that the box represents an entire â€œstackâ€ of these identically distributed variables. This box plate is called a plate, with the analogy that it represents a stack of identical plates.

\section{Factor Graph}

A factor graph is a bipartite graph representing the factorization of a function.

Each edge in graph defines a function

Definition
A factor graph is a bipartite graph representing the factorization of a function.

Related Readings
[1]: Factor Graph, wikipedia.org

\section{Inference}

This addresses the question of probabilistic inference: how a PGM can be used to answer questions.

Even though a PGM generally describes a very high dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate algorithms for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of the most commonly used exact and approximate algorithms are implemented and applied to a real-world problem.

\section{Learning}

This course addresses the question of learning: how a PGM can be learned from a data set of examples.

The course discusses the key problems of parameter estimation in both directed and undirected models, as well as the structure learning task for directed models. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of two commonly used learning algorithms are implemented and applied to a real-world problem.

\section{An Introduction to UnBBayes}

UnBBayes is a probabilistic network framework written in Java. It has both a GUI and an API with inference, sampling, learning and evaluation. It supports Bayesian networks, influence diagrams, MSBN, OOBN, HBN, MEBN/PR-OWL, PRM, structure, parameter and incremental learning.

Features
Probabilistic Networks:
Bayesian Network (BN)
Junction Tree
Likelihood Weighting
Gibbs
Influence Diagram (ID)
Multiply Sectioned Bayesian Network (MSBN)
Hybrid Bayesian Network (HBN)
Gaussian Mixture - Propagation under development
Object-Oriented Bayesian Network (OOBN)
FOL Probabilistic Network:
Multi-Entity Bayesian Network (MEBN)
Probabilistic Ontology Language (PR-OWL)
Learning Bayesian Network:
K2
B
CBL-A
CBL-B
Incremental Learning
Sampling
Logic
Likelihood Weighting
Gibbs
Classification Performance Evaluation
Evaluation using Logic Sampling
Evaluation using Likelihood Weighting Sampling
Installation
Go to https://sourceforge.net/projects/unbbayes/files/latest/download?source=typ_redirect to download zip file
Extract file unbbayes-4.21.18.zip to unbbayes-4.21.18 folder
Open unbbayes-4.21.18 folder, double click to unbbayes.bat
unbbayes-4.21.18 open

Official Videos
In this section, I add some official videos from unbbayes team. There are overview

Overview
In this video we are going to show the basic function we have in UnBBayes. This is the first of many tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.


Bayesian Network
In this video we are going to show how to create and compile a Bayesian Network (BN) in UnBBayes. This is our second of many video tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.


UnBBayes Performance Evaluation for Multi-Sensor Classification Systems
In this video we are going to show how to do a performance evaluation for multi-sensor classification systems in UnBBayes. It has been a while we do not post new videos, but hopefully this third one is just one more of many tutorials we will have available to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.


Probabilistic Ontology Modeling Using UnBBayes
In this video we discuss how to model probabilistic ontologies using PR-OWL/MEBN in UnBBayes. This session was a video conference between PhD students from the Institute of Business Administration (http://www.iba.edu.pk) and Rommel Carvalho from George Mason University (http://www.gmu.edu).

\section{Medical Domain Data}

We have provided you with a joint probability distribution of symptons, conditions and diseases based on the "flu" example in class. Certain diseases are more likely than others given certain symptons, and a model such as this can be used to help doctors make a diagnosis. (Don't actually use this for diagnosis, though!). The ground-truth joint probability distribution consists of twelve binary random variables and contains 212212 possible configurations (numbered 0 to 4095), which is small enough that you can enumerate them exhaustively. The variables are as follows:

(0) IsSummer true if it is the summer season, false otherwise.
(1) HasFlu true if the patient has the flu.
(2) HasFoodPoisoning true if the patient has food poisoning.
(3) HasHayFever true if patient has hay fever.
(4) HasPneumonia true if the patient has pneumonia.
(5) HasRespiratoryProblems true if the patient has problems in the respiratory system.
(6) HasGastricProblems true if the patient has problems in the gastro-intestinal system.
(7) HasRash true if the patient has a skin rash.
(8) Coughs true if the patient has a cough.
(9) IsFatigued true if the patient is tired and fatigued.
(10) Vomits true if the patient has vomited.
(11) HasFever true if the patient has a high fever.
You can download all the data here. The archive contains two files:

joint.dat: The true joint probability distribution over the twelve binary variables. Since each variable is binary, we can represent a * full variable assignment as a bitstring. This file lists all 2^12 assignments (one in each line) as pairs "Integer Probability" where "Integer" is an integer encoding of the bitstring. Specifically, assuming false=0 and true=1, an assignment to all variables results in a 12-bit binary number (with the index of the variables shown in parantheses above) which is converted to a decimal number. For example, assignment 0 represents all variables are false, 1 represents only IsSummer is true, 2 represents only HasFlu is true, and so on.
dataset.dat: The dataset consists of samples from the above probability distribution. Each line of the file contains a complete assignment to all the variables, encoded as an integer (as described above).

\section{Optical Word Recognition}

We will be studying the computer vision task of recognizing words from images. The task of recognizing words is usually decomposed to recognition of individual characters from their respective images (optical character recognition, OCR), and hence inferring the word. However character recognition is often a very difficult task, and since each character is predicted independent of its neighbors, its results can often contain combinations of characters that may not be possible in English. In this homework we will augment a simple OCR model with additional factors that capture some intuitions based on character co-occurences and image similarities.



The undirected graphical model for recognition of a given word is given in the figure above. It consists of two types of variables:

Image Variables: These are observed images that we need to predict the corresponsing character of, and the number of these image variables for a word is the number of characters in the word. The value of these image variables is an observed image, represented by an integer id (less than 1000). For the description of the model, assume the id of the image at position i is represented by img(i).
Character Variables: These are unobserved variables that represent the character prediction for each of the images, and there is one of these for each of the image variables. For our dataset, the domain of these variables is restricted to the ten most frequent characters in the English language ({e,t,a,o,i,n,s,h,r,d} [ciation]), instead of the complete alphabet. For the discussion below, assume the predicted character at position i is represented by char(i).
The model for a word w will consist of len(w) observed image ids, and the same number of unobserved character variables. For a given assignment to these character variables, the model score will be specified using three types of factors:

OCR Factors, ÏˆoÏˆo : These factors capture the predictions of a character-based OCR system, and hence exist between every image variable and its corresponding character variable. The number of these factors of word w is len(w). The value of factor between an image variable and the character variable at position i is dependent on img(i) and char(i), and is stored in ocr.dat file described in the data section.
Transition Factors, ÏˆtÏˆt : Since we also want to represent the co-occurence frequencies of the characters in our model, we add these factors between all consecutive character variables. The number of these factors of word w is len(w)-1. The value of factor between two character variables at positions i and i+1 is dependent on char(i) and char(i+1), and is high if char(i+1) is frequently preceded by char(i) in english words. These values are given to you in trans.dat file described in the data section.
Skip Factors, ÏˆsÏˆs : Another intuition that we would like to capture in our model is that similar images in a word always represent the same character. Thus our model score should be higher if it predicts the same characters for similar images. These factors exist between every pair of image variables that have the same id, i.e. this factor exist between all i,j, i!=j such that img(i)==img(j). The value of this factor depends on char(i) and char(j), and is 5.0 if char(i)==char(j), and 1.0 otherwise.
You can download all the data here. The archive contains the following files:

ocr.dat: Contains the output predictions of a pre-existing OCR system for the set of thousand images. Each row contains three tab separated values "id a prob" and represents the OCR system's probability that image id represents character aa, p(char=a|img=id)=probp(char=a|img=id)=prob. Use these values directly as the value of the factor between image and character variables at position ii, Ïˆo(image(i)=id,char(i)=a)=probÏˆo(image(i)=id,char(i)=a)=prob. Since there are 10 characters and 1000 images, the total number of rows in this file is 10,000.
trans.dat: Stores the factor potentials for the transition factors. Each row contains three tab-separated values "a b value" that represents the value of factor when the previous character is "a" and the next character is "b", i.e. (char(i)=a, char(i+1)=b) = value. The number of rows in the file is 100 (10*10). data.dat (and truth.dat): Dataset to run your experiments on (see Core Tasks below). The observed dataset (data.dat) consists observed images of one word on each row. The observed images for a word are represented by a sequence of tab-separated integer ids ("id1 id2 id3"). The true word for these observed set of images is stored the respective row in truth.dat, and is simply a string ("eat"). For the core task (3) below, you should iterate through both the files together to ensure you have the true word along with the observed images.
Extra files (bicounts.dat, allwords.dat, allimagesX.dat): These files are not necessary for the core tasks, but may be useful for further fun and your own exploration. allwords.dat and allimagesX.dat are larger versions of data.dat and truth.dat, i.e. they contain all possible words that can be generated from our restricted set of alphabet, and five samples of their observed image sequences (one in each file). You can run inference on these if you like, but is likely to take 15-20 times longer than the small dataset. bicount.dat is in the same format as trans.dat, but instead of storing inexplicable potentials, it stores the joint probability of the co-occurences of the characters.
Core Task
1. Graphical Model: Implement the graphical model containing the factors above. For any given assignment to the character variables, your model should be able to calculate the model score. Implemention should allow switching between three models:

OCR model: only contains the OCR factors
Transition model: contains OCR and Transition factors
Combined model: containing all three types of factors
Note: To avoid errors arising from numerical issues, we suggest you represent the factors in the log-space and take sums as much as possible, calculating the log of the model score.

2. Exhaustive Inference: Using the graphical model, write code to perform exhaustive inference, i.e. your code should be able to calculate the probability of any assignment of the character and image variables. To calculate the normalization constant Z for the word w, you will need to go through all possible assignments to the character variables (there will be 10len(w)10len(w) of these).

3. Model Accuracy: Run your model on the data given in the file data.dat. For every word in the dataset, pick the assignment to character variables that has the highest probability according to the model, and treat this as the model prediction for the word. Using the truth given in truth.dat, compare the accuracy of the model predictions using the following three metrics: 1. Character-wise accuracy: Ratio of correctly predicted characters to total number of characters 2. Word-wise accuracy: Ratio of correctly predicted words to total number of words 3. Average Dataset log-likelihood: For each word given in data.dat, calculate the log of the probability of the true word according to the model. Compute the average of this value for the whole dataset.

Compare all of the three models described in (1) using these three metrics. Also give some examples of words that were incorrect by the OCR model but consequently fixed by the Transition model, and examples of words that were incorrect by the OCR, partially corrected by the Transition model, and then completely fixed by the Combined model.



