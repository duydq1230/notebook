\chapter{Probabilistic Graphical Model}

View online \href{http://magizbox.com/training/probabilistic_graphical_models/site/}{http://magizbox.com/training/probabilistic_graphical_models/site/}

Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.

\section{Representation}

Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other.

These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems.

\section{Foundation: Graph}

Perhaps the most pervasive concept in this book is the representation of a probability distribution using a graph as a data structure. In this section, we survey some of the basic concepts in graph theory used in the book.

1 Nodes and Edges
A graph is a data structure K consisting of a set of nodes and a set of edges. Throughout most this book, we will assume that the set of nodes is X = {X1,...,Xn}. A pair of nodes Xi,Xj directed edge can be connected by a directed edge Xi â†’ Xj or an undirected edge Xiâ€”Xj. Thus, the set undirected edge of edges E is a set of pairs, where each pair is one of Xi â†’ Xj, Xj â†’ Xi, or Xiâ€”Xj, for Xi,Xj âˆˆ X , i < j. We assume throughout the book that, for each pair of nodes Xi,Xj, at most one type of edge exists; thus, we cannot have both Xi â†’ Xj and Xj â†’ Xi, nor can we have Xi â†’ Xj and Xiâ€”Xj.2 The notation Xi â† Xj is equivalent to Xj â†’ Xi, and the notation X jâ€”Xi is equivalent to Xiâ€”Xj. We use Xi Xj to represent the case where Xi and X j are connected via some edge, whether directed (in any direction) or undirected. In many cases, we want to restrict attention to graphs that contain only edges of one kind directed graph or another. We say that a graph is directed if all edges are either Xi â†’ Xj or Xj â†’ Xi. We usually denote directed graphs as G. We say that a graph is undirected if all edges are Xiâ€”Xj. undirected graph We denote undirected graphs as H. We sometimes convert a general graph to an undirected graph by ignoring the directions on the edges. Definition 2.11 Given a graph K = (X, E), its undirected version is a graph H = (X, E0) where E0 = {Xâ€”Y : graphâ€™s undirected version X Y âˆˆ E}. Whenever we have that Xi â†’ Xj âˆˆ E, we say that Xj is the child of Xi in K, and that child Xi is the parent of Xj in K. When we have Xiâ€”Xj âˆˆ E, we say that Xi is a neighbor of parent neighbor Xj in K (and vice versa). We say that X and Y are adjacent whenever X Y âˆˆ E. We use PaX to denote the parents of X, ChX to denote its children, and NbX to denote its neighbors. We define the boundary of X, denoted BoundaryX, to be PaX âˆª NbX; for DAGs, this set is boundary simply Xâ€™s parents, and for undirected graphs Xâ€™s neighbors.3 Figure 2.3 shows an example of a graph K. There, we have that A is the only parent of C, and F,I are the children of C. The degree only neighbor of C is D, but its adjacent nodes are A,D,F,I. The degree of a node X is the number of edges in which it participates. Its indegree is the number of directed edges Y â†’ X. indegree The degree of a graph is the maximal degree of a node in the graph. 2. Note that our definition is somewhat restricted, in that it disallows cycles of length two, where Xi â†’ Xj â†’ Xi, and allows self-loops where Xi â†’ Xi. 3. When the graph is not clear from context, we often add the graph as an additional argument.


2 Subgraphs
In many cases, we want to consider only the part of the graph that is associated with a particular subset of the nodes. Definition 2.12 Let K = (X , E), and let X âŠ‚ X . We define the induced subgraph K[X] to be the graph (X, E0) induced subgraph where E0 are all the edges X Y âˆˆ E such that X, Y âˆˆ X. For example, figure 2.4a shows the induced subgraph K[C, D, I]. A type of subgraph that is often of particular interest is one that contains all possible edges. Definition 2.13 A subgraph over X is complete if every two nodes in X are connected by some edge. The set X complete subgraph is often called a clique; we say that a clique X is maximal if for any superset of nodes Y âŠƒ X, clique Y is not a clique. Although the subset of nodes X can be arbitrary, we are often interested in sets of nodes that preserve certain aspects of the graph structure. Definition 2.14 We say that a subset of nodes X âˆˆ X is upwardly closed in K if, for any X âˆˆ X, we have that upward closure BoundaryX âŠ‚ X. We define the upward closure of X to be the minimal upwardly closed subset

Y that contains X. We define the upwardly closed subgraph of X, denoted K+[X], to be the induced subgraph over Y , K[Y ]. For example, the set A, B, C, D, E is the upward closure of the set {C} in K. The upwardly closed subgraph of {C} is shown in figure 2.4b. The upwardly closed subgraph of {C, D, I} is shown in figure 2.4c.

3 Paths and Trails
Using the basic notion of edges, we can define dierent types of longer-range connections in the graph.

Definition path

We say that X1,...,XkX1,...,Xk form a path in the graph K=(X,E)K=(X,E) if, for every i=1,...,kâˆ’1i=1,...,kâˆ’1, we have that either Xiâ†’Xi+1Xiâ†’Xi+1 or Xiâˆ’Xi+1Xiâˆ’Xi+1. A path is directed if, for at least one i, we have Xiâ†’Xi+1Xiâ†’Xi+1.

Definition trail

We say that X1,...,XkX1,...,Xk form a trail in the graph K=(X,E)K=(X,E) if, for every i=1,...,kâˆ’1i=1,...,kâˆ’1, we have that Xiâ‡ŒXi+1Xiâ‡ŒXi+1.


In the graph KK of figure 2.3, A,C,D,E,IA,C,D,E,I is a path, and hence also a trail. On the other hand, A,C,F,G,DA,C,F,G,D is a trail, which is not a path.

Definition connected graph

A graph is connected if for every Xi,XjXi,Xj there is a trail between XiXi and XjXj.

We can now define longer-range relationships in the graph.

Definition ancestor, descendant

We say that XX is an ancestor of YY in K=(X,E)K=(X,E), and that YY is a descendant of XX, if there exists a directed path X1,...,XkX1,...,Xk with X1=XX1=X and Xk=YXk=Y. We use DescendantsXDescendantsX to denote Xâ€™s descendants, AncestorsXAncestorsX to denote Xâ€™s ancestors, and NonDescendantsXNonDescendantsX to denote the set of nodes in Xâˆ’DescendantsXXâˆ’DescendantsX.

In our example graph K, we have that F,G,IF,G,I are descendants of CC. The ancestors of CC are AA, via the path A,C,A,C, and BB, via the path B,E,D,CB,E,D,C.

A final useful notion is that of an ordering of the nodes in a directed graph that is consistent with the directionality its edges.

Definition topological ordering

Let G=(X,E)G=(X,E) be a graph. An ordering of the nodes X1,...,XnX1,...,Xn is a topological ordering relative to KK if, whenever we have Xiâ†’XjâˆˆEXiâ†’XjâˆˆE, then i<ji<j.

Appendix A.3.1 presents an algorithm for finding such a topological ordering.

4 Cycles and Loops
Note that, in general, we can have a cyclic path that leads from a node to itself, making that node its own descendant.

Definition 2.20 A cycle in K is a directed path X1, . . . , Xk where X1 = Xk. A graph is acyclic if it contains no cycle acyclic cycles. For most of this book, we will restrict attention to graphs that do not allow such cycles, since it is quite dicult to define a coherent probabilistic model over graphs with directed cycles. DAG A directed acyclic graph (DAG) is one of the central concepts in this book, as DAGs are the basic graphical representation that underlies Bayesian networks. For some of this book, we also use acyclic graphs that are partially directed. The graph K of figure 2.3 is acyclic. However, if we add the undirected edge Aâ€”E to K, we have a path A, C, D, E, A from A to itself. Clearly, adding a directed edge E â†’ A would also lead to a cycle. Note that prohibiting cycles does not imply that there is no trail from a node to itself. For example, K contains several trails: C, D, E, I, C as well as C, D, G, F, C. An acyclic graph containing both directed and undirected edges is called a partially directed PDAG acyclic graph or PDAG. The acyclicity requirement on a PDAG implies that the graph can be chain component decomposed into a directed graph of chain components, where the nodes within each chain component are connected to each other only with undirected edges. The acyclicity of a PDAG guarantees us that we can order the components so that all edges point from lower-numbered components to higher-numbered ones. Definition 2.21 Let K be a PDAG over X . Let K1, . . . , K` be a disjoint partition of X such that: â€¢ the induced subgraph over Ki contains no directed edges; â€¢ for any pair of nodes X âˆˆ Ki and Y âˆˆ Kj for i < j, an edge between X and Y can only be a directed edge X â†’ Y . chain component Each component Ki is called a chain component. chain graph Because of its chain structure, a PDAG is also called a chain graph. Example 2.6 In the PDAG of figure 2.3, we have six chain components: {A}, {B}, {C, D, E}, {F, G}, {H}, and {I}. This ordering of the chain components is one of several possible legal orderings. Note that when the PDAG is an undirected graph, the entire graph forms a single chain component. Conversely, when the PDAG is a directed graph (and therefore acyclic), each node in the graph is its own chain component.


Dierent from a cycle is the notion of a loop: Definition 2.22 A loop in K is a trail X1, . . . , Xk where X1 = Xk. A graph is singly connected if it contains loop singly connected no loops. A node in a singly connected graph is called a leaf if it has exactly one adjacent node. leaf A singly connected directed graph is also called a polytree. A singly connected undirected graph is polytree called a forest; if it is also connected, it is called a tree. forest tree We can also define a notion of a forest, or of a tree, for directed graphs. Definition 2.23 A directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is also connected. Note that polytrees are very dierent from trees. For example, figure 2.5 shows a graph that is a polytree but is not a tree, because several nodes have more than one parent. As we will discuss later in the book, loops in the graph increase the computational cost of various tasks. We conclude this section with a final definition relating to loops in the graph. This definition will play an important role in evaluating the cost of reasoning using graph-based representations. Definition 2.24 Let X1â€”X2â€” Â· Â· Â· â€”Xkâ€”X1 be a loop in the graph; a chord in the loop is an edge connecting chordal graph Xi and Xj for two nonconsecutive nodes Xi, Xj. An undirected graph H is said to be chordal if any loop X1â€”X2â€” Â· Â· Â· â€”Xkâ€”X1 for k â‰¥ 4 has a chord. Thus, for example, a loop Aâ€”Bâ€”Câ€”Dâ€”A (as in figure 1.1b) is nonchordal, but adding an edge Aâ€”C would render it chordal. In other words, in a chordal graph, the longest â€œminimal loopâ€ (one that has no shortcut) is a triangle. Thus, chordal graphs are often also called triangulated triangulated. graph We can extend the notion of chordal graphs to graphs that contain directed edges. Definition 2.25 A graph K is said to be chordal if its underlying undirected graph is chordal.

\section{Bayesian Network}

A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.

A Non-Causal Bayesian Network Example
Figure 1 shows a simple Bayesian network, which consists of only two nodes and one link. It represents the JPD of the variables Eye Color and Hair Color in a population of students (Snee, 1974). In this case, the conditional probabilities of Hair Color given the values of its parent node, Eye Color, are provided in a CPT. It is important to point out that this Bayesian network does not contain any causal assumptions, i.e. we have no knowledge of the causal order between the variables. Thus, the interpretation of this network should be merely statistical (informational).



A Causal Network Example
Figure 2 illustrates another simple yet typical Bayesian network. In contrast to the statistical relationships in Figure 1, the diagram in Figure 2 describes the causal relationships among the seasons of the year (X1X1), whether it is raining (X2X2), whether the sprinkler is on (X3X3), whether the pavement is wet (X4X4), and whether the pavement is slippery (X5X5). Here, the absence of a direct link between X1X1 and X5X5, for example, captures our understanding that there is no direct influence of season on slipperiness. The influence is mediated by the wetness of the pavement (if freezing were a possibility, a direct link could be added).



A Dynamic Bayesian Network Example
Entities that live in a changing environment must keep track of variables whose values change over time. Dynamic Bayesian networks capture this process by representing multiple copies of the state variables, one for each time step. A set of variables Xtâˆ’1Xtâˆ’1 and XtXt denotes the world state at times t-1 and t respectively. A set of evidence variables Et denotes the observations available at time t. The sensor model P(Et|Xt)P(Et|Xt) is encoded in the conditional probability distributions for the observable variables, given the state variables. The transition model P(Xt|Xtâˆ’1)P(Xt|Xtâˆ’1) relates the state at time t-1 to the state at time t. Keeping track of the world means computing the current probability distribution over world states given all past observations, i.e. P(Xt|E1,â€¦,Et)P(Xt|E1,â€¦,Et).

Dynamic Bayesian networks (DBN) are a generalization of Hidden Markov Models (HMM) and Kalman Filters (KF). Every HMM and KF can be represented with a DBN. Furthermore, the DBN representation of an HMM is much more compact and, thus, much better understandable. The nodes in the HMM represent the states of the system, whereas the nodes in the DBN represent the dimensions of the system. For example, the HMM representation of the valve system in Figure 2.3 is made of 26 nodes and 36 arcs, versus 9 nodes and 11 arcs in the DBN (Weber and Jouffe, 2003).

\section{Template Models for Bayesian Networks}

In many cases, we need to model distributions that have a recurring structure. In this module, we describe representations for two such situations. One is temporal scenarios, where we want to model a probabilistic structure that holds constant over time; here, we use Hidden Markov Models, or, more generally, Dynamic Bayesian Networks. The other is aimed at scenarios that involve multiple similar entities, each of whose properties is governed by a similar model; here, we use Plate Models.

Temporal Models
Our focus in this section is on modeling dynamic settings, where we are interested in reasoning about the state of the world as it evolves over time. We can model such settings in terms of a system state system state, whose value at time t is a snapshot of the relevant attributes (hidden or observed) of the system at time t. We assume that the system state is represented, as usual, as an assignment of values to some set of random variables X . We use X (t) i to represent the instantiation of the variable Xi at time t. Note that Xi itself is no longer a variable that takes a value; rather, it is a template variable template variable. This template is instantiated at dierent points in time t, and each Xi (t) is a variable that takes a value in Val(Xi). For a set of variables X âŠ† X , we use X (t1:t2) (t1 < t2) to denote the set of variables {X (t) : t âˆˆ [t1,t2]}. As usual, we use the notation x(t:t0) for an assignment of values to this set of variables.

Each â€œpossible worldâ€ in our probability space is now a trajectory: an assignment of values to each variable X (t) i for each relevant time t. Our goal therefore is to represent a joint distribution over such trajectories. Clearly, the space of possible trajectories is a very complex probability space, so representing such a distribution can be very dicult. We therefore make a series of simplifying assumptions that help make this representational problem more tractable.

Dynamic Bayesian Networks




Directed Probabilistic Models for Object-Relational Domains
Based on the framework described in the previous section, we now describe template-based representation languages that can encode directed probabilistic models.

Plate Models
We begin our discussion by presenting the plate model, the simplest and best-established of the object-relational frameworks. Although restricted in several important ways, the plate modeling framework is perhaps the approach that has been most commonly used in practice, notably for encoding the assumptions made in various learning tasks. This framework also provides an excellent starting point for describing the key ideas of template-based languages and for motivating some of the extensions that have been pursued in richer languages.

In the plate formalism, object types are called plates. The fact that multiple objects in the class share the same set of attributes and same probabilistic model is the basis for the use of the term â€œplate,â€ which suggests a stack of identical objects. We begin with some motivating examples and then describe the formal framework.

Examples
Example 1 The simplest example of a plate model, shown in figure 6.6, describes multiple random variables generated from the same distribution. In this case, we have a set of random variables X(d) (dâˆˆD)X(d) (dâˆˆD) that all have the same domain Val(X) and are sampled from the same distribution. In a plate representation, we encode the fact that these variables are all generated from the same template by drawing only a single node X(d) and enclosing it in a box denoting that d ranges over D, so that we know that the box represents an entire â€œstackâ€ of these identically distributed variables. This box plate is called a plate, with the analogy that it represents a stack of identical plates.

\section{Factor Graph}

A factor graph is a bipartite graph representing the factorization of a function.

Each edge in graph defines a function

Definition
A factor graph is a bipartite graph representing the factorization of a function.

Related Readings
[1]: Factor Graph, wikipedia.org

\section{Conditional Random Fields}

CRFs lÃ  mÃ´ hÃ¬nh tráº¡ng thÃ¡i tuyáº¿n tÃ­nh vÃ´ hÆ°á»›ng (mÃ¡y tráº¡ng thÃ¡i há»¯u háº¡n Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ³ Ä‘ieuÃ¨ kiá»‡n) vÃ  tuÃ¢n theo tÃ­nh cháº¥t Markov thá»© nháº¥t. CRFs Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh ráº¥t thÃ nh cÃ´ng cho cÃ¡c bÃ i toÃ¡n gÃ¡n nhÃ£n chuá»—i nhÆ° tÃ¡ch tá»«, gÃ¡n nhÃ£n cá»¥m tá»«, xÃ¡c Ä‘á»‹nh thá»±c thá»ƒ, gÃ¡n nhÃ£n cá»¥m danh tá»«, etc.

Gá»i o lÃ  má»™t chuá»—i dá»± lieuáº¹ quan sÃ¡t cáº§n Ä‘Æ°á»£c gÃ¡n nhÃ£n. Gá»i S lÃ  táº­p trÃ¡ng tháº¡i, má»—i tráº¡ng thÃ¡i liÃªn káº¿t vá»›i má»™t nhÃ£n l. Äáº·t s lÃ  má»™t chuá»—i tráº¡ng thÃ¡i nÃ o Ä‘Ã³, CRFs xÃ¡c Ä‘á»‹nh xÃ¡c suáº¥t Ä‘iá»u kiá»‡n cá»§a má»™t chuá»—i tráº¡ng thÃ¡i khi biáº¿t chuá»—i quan sÃ¡t nhÆ° sau:

Gá»i Z(o) lÃ  thá»«a sá»‘ chuáº©n hÃ³a trÃªn bá»™ cÃ¡c chuá»—i nhÃ£n cÃ³ thá»ƒ. fk xÃ¡c Ä‘á»‹nh má»™t hÃ m Ä‘áº·c trÆ°ng vÃ  lambda k lÃ  trá»ng sá»‘ liÃªn káº¿t vá»›i má»—i Ä‘áº·c trÆ°ng fk. Má»¥c Ä‘á»‹ch cá»§a viá»‡c há»c mÃ¡y vá»›i CRFs lÃ  Æ°á»›c lÆ°á»£ng cÃ¡c trá»ng sá»‘ nÃ y. á»Ÿ Ä‘Ã¢y, cÃ³ hai loáº¡i Ä‘áº·c trÆ°ng fk: Ä‘áº·c trÆ°ng tráº¡ng thÃ¡i (per-state) vÃ  Ä‘áº·c trÆ°ng chuyá»ƒn (transition)

fk

fk

á»ž Ä‘áº­y $\sigma$ lÃ  Kronecker-sigma. Má»—i Ä‘áº·c trÆ°ng tráº¡ng thÃ¡i (2) káº¿t há»£p nhÃ£n l cá»§a tráº¡ng thÃ¡i hienáº¹ táº¡i $s_t$ lÃ  má»™t vá»‹ tá»« ngá»¯ cáº£nh - má»™t hÃ m nhá»‹ nhanáº¡ xk xÃ¡c Ä‘á»‹nh cÃ¡c ngá»¯ cáº£nh quan trá»ng cá»§a quan sÃ¡t o táº¡i vá»‹ trÃ­ t. Má»—i Ä‘áº·c trÆ°ng chuyá»ƒn biá»ƒu diá»…n sá»± phá»¥ thuá»™c chuá»—i báº±ng cÃ¡ch ketÃ© há»£p nhÃ£n l cá»§a tráº¡ng thÃ¡i trÆ°á»›c s vÃ  nhÃ£n l cá»§a tráº¡ng thÃ¡i hienáº¹ táº¡i s.

NgÆ°á»i ta thÆ°á»ng huáº¥n luyenáº¹ CRFs báº±ng cÃ¡ch lÃ m cá»±c Ä‘áº¡i hÃ³a hÃ m likelihood theo dá»¯ lieuáº¹ huáº¥n luyá»‡n sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u nhÆ° L-BFGS. Viá»‡c láº­p luanáº¡ (dá»±a trÃªn mÃ´ hÃ¬nh Ä‘Ã£ há»c) lÃ  tÃ¬m ra chuá»—i nhÃ£n tÆ°Æ¡ng á»©ng cá»§a má»™t chuá»—i quan sÃ¡t Ä‘áº§u vÃ o. Äá»‘i vá»›i CRFs, ngÆ°á»i ta thÆ°á»ng sá»­ dá»¥ng thuáº­t toÃ¡n quy hoáº¡ch Ä‘á»™ng Ä‘iá»ƒn hÃ¬nh lÃ  Viterbi Ä‘á»ƒ thá»±c hiá»‡n láº­p luáº­n vá»›i dá»¯ liá»‡u má»›i.

Tai lieu tham khao

http://www.jaist.ac.jp/~bao/VLSP-text/ICTrda08/ICT08-VLSP-SP83.pdf


\section{Inference}

This addresses the question of probabilistic inference: how a PGM can be used to answer questions.

Even though a PGM generally describes a very high dimensional distribution, its structure is designed so as to allow questions to be answered efficiently. The course presents both exact and approximate algorithms for different types of inference tasks, and discusses where each could best be applied. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of the most commonly used exact and approximate algorithms are implemented and applied to a real-world problem.

\section{Learning}

This course addresses the question of learning: how a PGM can be learned from a data set of examples.

The course discusses the key problems of parameter estimation in both directed and undirected models, as well as the structure learning task for directed models. The (highly recommended) honors track contains two hands-on programming assignments, in which key routines of two commonly used learning algorithms are implemented and applied to a real-world problem.

\section{An Introduction to UnBBayes}

UnBBayes is a probabilistic network framework written in Java. It has both a GUI and an API with inference, sampling, learning and evaluation. It supports Bayesian networks, influence diagrams, MSBN, OOBN, HBN, MEBN/PR-OWL, PRM, structure, parameter and incremental learning.

Features
Probabilistic Networks:
Bayesian Network (BN)
Junction Tree
Likelihood Weighting
Gibbs
Influence Diagram (ID)
Multiply Sectioned Bayesian Network (MSBN)
Hybrid Bayesian Network (HBN)
Gaussian Mixture - Propagation under development
Object-Oriented Bayesian Network (OOBN)
FOL Probabilistic Network:
Multi-Entity Bayesian Network (MEBN)
Probabilistic Ontology Language (PR-OWL)
Learning Bayesian Network:
K2
B
CBL-A
CBL-B
Incremental Learning
Sampling
Logic
Likelihood Weighting
Gibbs
Classification Performance Evaluation
Evaluation using Logic Sampling
Evaluation using Likelihood Weighting Sampling
Installation
Go to https://sourceforge.net/projects/unbbayes/files/latest/download?source=typ_redirect to download zip file
Extract file unbbayes-4.21.18.zip to unbbayes-4.21.18 folder
Open unbbayes-4.21.18 folder, double click to unbbayes.bat
unbbayes-4.21.18 open

Official Videos
In this section, I add some official videos from unbbayes team. There are overview

Overview
In this video we are going to show the basic function we have in UnBBayes. This is the first of many tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.


Bayesian Network
In this video we are going to show how to create and compile a Bayesian Network (BN) in UnBBayes. This is our second of many video tutorials we have been creating to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.


UnBBayes Performance Evaluation for Multi-Sensor Classification Systems
In this video we are going to show how to do a performance evaluation for multi-sensor classification systems in UnBBayes. It has been a while we do not post new videos, but hopefully this third one is just one more of many tutorials we will have available to support the demand for documentation on how to use UnBBayes. We hope this will help UnBBayes' user community to grow even more.


Probabilistic Ontology Modeling Using UnBBayes
In this video we discuss how to model probabilistic ontologies using PR-OWL/MEBN in UnBBayes. This session was a video conference between PhD students from the Institute of Business Administration (http://www.iba.edu.pk) and Rommel Carvalho from George Mason University (http://www.gmu.edu).

\section{Medical Domain Data}

We have provided you with a joint probability distribution of symptons, conditions and diseases based on the "flu" example in class. Certain diseases are more likely than others given certain symptons, and a model such as this can be used to help doctors make a diagnosis. (Don't actually use this for diagnosis, though!). The ground-truth joint probability distribution consists of twelve binary random variables and contains 212212 possible configurations (numbered 0 to 4095), which is small enough that you can enumerate them exhaustively. The variables are as follows:

(0) IsSummer true if it is the summer season, false otherwise.
(1) HasFlu true if the patient has the flu.
(2) HasFoodPoisoning true if the patient has food poisoning.
(3) HasHayFever true if patient has hay fever.
(4) HasPneumonia true if the patient has pneumonia.
(5) HasRespiratoryProblems true if the patient has problems in the respiratory system.
(6) HasGastricProblems true if the patient has problems in the gastro-intestinal system.
(7) HasRash true if the patient has a skin rash.
(8) Coughs true if the patient has a cough.
(9) IsFatigued true if the patient is tired and fatigued.
(10) Vomits true if the patient has vomited.
(11) HasFever true if the patient has a high fever.
You can download all the data here. The archive contains two files:

joint.dat: The true joint probability distribution over the twelve binary variables. Since each variable is binary, we can represent a * full variable assignment as a bitstring. This file lists all 2^12 assignments (one in each line) as pairs "Integer Probability" where "Integer" is an integer encoding of the bitstring. Specifically, assuming false=0 and true=1, an assignment to all variables results in a 12-bit binary number (with the index of the variables shown in parantheses above) which is converted to a decimal number. For example, assignment 0 represents all variables are false, 1 represents only IsSummer is true, 2 represents only HasFlu is true, and so on.
dataset.dat: The dataset consists of samples from the above probability distribution. Each line of the file contains a complete assignment to all the variables, encoded as an integer (as described above).

\section{Optical Word Recognition}

We will be studying the computer vision task of recognizing words from images. The task of recognizing words is usually decomposed to recognition of individual characters from their respective images (optical character recognition, OCR), and hence inferring the word. However character recognition is often a very difficult task, and since each character is predicted independent of its neighbors, its results can often contain combinations of characters that may not be possible in English. In this homework we will augment a simple OCR model with additional factors that capture some intuitions based on character co-occurences and image similarities.



The undirected graphical model for recognition of a given word is given in the figure above. It consists of two types of variables:

Image Variables: These are observed images that we need to predict the corresponsing character of, and the number of these image variables for a word is the number of characters in the word. The value of these image variables is an observed image, represented by an integer id (less than 1000). For the description of the model, assume the id of the image at position i is represented by img(i).
Character Variables: These are unobserved variables that represent the character prediction for each of the images, and there is one of these for each of the image variables. For our dataset, the domain of these variables is restricted to the ten most frequent characters in the English language ({e,t,a,o,i,n,s,h,r,d} [ciation]), instead of the complete alphabet. For the discussion below, assume the predicted character at position i is represented by char(i).
The model for a word w will consist of len(w) observed image ids, and the same number of unobserved character variables. For a given assignment to these character variables, the model score will be specified using three types of factors:

OCR Factors, ÏˆoÏˆo : These factors capture the predictions of a character-based OCR system, and hence exist between every image variable and its corresponding character variable. The number of these factors of word w is len(w). The value of factor between an image variable and the character variable at position i is dependent on img(i) and char(i), and is stored in ocr.dat file described in the data section.
Transition Factors, ÏˆtÏˆt : Since we also want to represent the co-occurence frequencies of the characters in our model, we add these factors between all consecutive character variables. The number of these factors of word w is len(w)-1. The value of factor between two character variables at positions i and i+1 is dependent on char(i) and char(i+1), and is high if char(i+1) is frequently preceded by char(i) in english words. These values are given to you in trans.dat file described in the data section.
Skip Factors, ÏˆsÏˆs : Another intuition that we would like to capture in our model is that similar images in a word always represent the same character. Thus our model score should be higher if it predicts the same characters for similar images. These factors exist between every pair of image variables that have the same id, i.e. this factor exist between all i,j, i!=j such that img(i)==img(j). The value of this factor depends on char(i) and char(j), and is 5.0 if char(i)==char(j), and 1.0 otherwise.
You can download all the data here. The archive contains the following files:

ocr.dat: Contains the output predictions of a pre-existing OCR system for the set of thousand images. Each row contains three tab separated values "id a prob" and represents the OCR system's probability that image id represents character aa, p(char=a|img=id)=probp(char=a|img=id)=prob. Use these values directly as the value of the factor between image and character variables at position ii, Ïˆo(image(i)=id,char(i)=a)=probÏˆo(image(i)=id,char(i)=a)=prob. Since there are 10 characters and 1000 images, the total number of rows in this file is 10,000.
trans.dat: Stores the factor potentials for the transition factors. Each row contains three tab-separated values "a b value" that represents the value of factor when the previous character is "a" and the next character is "b", i.e. (char(i)=a, char(i+1)=b) = value. The number of rows in the file is 100 (10*10). data.dat (and truth.dat): Dataset to run your experiments on (see Core Tasks below). The observed dataset (data.dat) consists observed images of one word on each row. The observed images for a word are represented by a sequence of tab-separated integer ids ("id1 id2 id3"). The true word for these observed set of images is stored the respective row in truth.dat, and is simply a string ("eat"). For the core task (3) below, you should iterate through both the files together to ensure you have the true word along with the observed images.
Extra files (bicounts.dat, allwords.dat, allimagesX.dat): These files are not necessary for the core tasks, but may be useful for further fun and your own exploration. allwords.dat and allimagesX.dat are larger versions of data.dat and truth.dat, i.e. they contain all possible words that can be generated from our restricted set of alphabet, and five samples of their observed image sequences (one in each file). You can run inference on these if you like, but is likely to take 15-20 times longer than the small dataset. bicount.dat is in the same format as trans.dat, but instead of storing inexplicable potentials, it stores the joint probability of the co-occurences of the characters.
Core Task
1. Graphical Model: Implement the graphical model containing the factors above. For any given assignment to the character variables, your model should be able to calculate the model score. Implemention should allow switching between three models:

OCR model: only contains the OCR factors
Transition model: contains OCR and Transition factors
Combined model: containing all three types of factors
Note: To avoid errors arising from numerical issues, we suggest you represent the factors in the log-space and take sums as much as possible, calculating the log of the model score.

2. Exhaustive Inference: Using the graphical model, write code to perform exhaustive inference, i.e. your code should be able to calculate the probability of any assignment of the character and image variables. To calculate the normalization constant Z for the word w, you will need to go through all possible assignments to the character variables (there will be 10len(w)10len(w) of these).

3. Model Accuracy: Run your model on the data given in the file data.dat. For every word in the dataset, pick the assignment to character variables that has the highest probability according to the model, and treat this as the model prediction for the word. Using the truth given in truth.dat, compare the accuracy of the model predictions using the following three metrics: 1. Character-wise accuracy: Ratio of correctly predicted characters to total number of characters 2. Word-wise accuracy: Ratio of correctly predicted words to total number of words 3. Average Dataset log-likelihood: For each word given in data.dat, calculate the log of the probability of the true word according to the model. Compute the average of this value for the whole dataset.

Compare all of the three models described in (1) using these three metrics. Also give some examples of words that were incorrect by the OCR model but consequently fixed by the Transition model, and examples of words that were incorrect by the OCR, partially corrected by the Transition model, and then completely fixed by the Combined model.



