\chapter{Học sâu}
\info{24/11/2017 - Từ hôm nay, mỗi ngày sẽ ghi chú một phần (rất rất nhỏ) về Deep Learning}
\info{22/11/2017 - Phải nói quyển này hơi nặng so với mình. Nhưng thôi cứ cố gắng vậy.}

Tài liệu cũ: \href{http://magizbox.com/training/deep_learning/site/}{http://magizbox.com/training/deep\_learning/site/}

Deep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals: Artificial Intelligence.

\section{Deep Feedforward Networks}

\definition{deep feedforward networks}{
Deep feedforward networks, (hay còn gọi là feedforward neural networks) hoặc multilayer perceptrons (MLPs) là mô hình deep learning cơ bản nhất. Mục tiêu của một feedforward network là xấp xỉ một hàm $f*$. Ví dụ, với bài toán phân loại, $y=f*(x)$ chuyển đầu vào x thành một nhãn y. Một feedforward network định nghĩa một ánh xạ $y=f(x, \theta)$ và học giá trị của $\theta$ để xấp xỉ hàm $f*$ tốt nhất.
}

Mô hình được gọi là feedforward vì giá trị của x đang lan truyền qua mạng đến output, mà không có chiều ngược lại. Các mô hình neural có sự lan truyền ngược lại được gọi là recurrent neural network.

Feedforward là nền tảng cho rất nhiều mô hình mạng neural hiện đại. Ví dụ, các ứng dụng trong việc nhận diện ảnh thường áp dụng mô hình convolutional, là một dạng đặc biệt của feedforward network. Feedforward network cũng là nền tảng cho mạng recurrent network, có nhiều ứng trọng xử lý ngôn ngữ.

Feedforward neural networks được gọi là network vì nó thường được biểu diễn bởi một đồ thị gồm nhiều tầng, định nghĩa các hàm được kết hợp với nhau như thế nào. Ví dụ, chúng ta có 3 hàm f(1)(x), f(2)(x), f(3)(x) được đặt với nhau thành một chuỗi, hình thành hàm f(x) = f(3)(f(2)(f(1)(x))). Cấu trúc chuỗi này là dạng phổ biến nhất trong mạng neural. Trong trường hợp này f(1)(x) được gọi là layer 1, f(2)(x) được gọi là layer 2, và cứ tiếp tục như vậy. Độ dài của chuỗi thể hiện độ sâu của mô hình. Thuật ngữ "deep learning" xuất phát từ điều này. Layer cuối cũng được goi là output layer.
Trong mạng neural, mục tiêu là học hàm f*(x). Ứng với mỗi giá trị đầu vào x, sẽ có tương ứng một giá trị y. Mục tiêu của mạng là tìm các tham số để y xấp xỉ với f*(x). Nhưng việc học của hàm không hoàn toàn phụ thuộc vào x, y, mà do learning algorithm quyết định. Vì dữ liệu huấn luyện không chỉ rõ giá trị ở những layer ở giữa, nên chúng được gọi là các hidden layers
Cuối cùng, mạng neural được gọi là neural vì nó lấy cảm hứng từ ngành khoa học thần kinh. Mỗi hidden layer trong mạng thường là các giá trị vector. Mỗi phần tử trong vector sẽ quyết định việc hành xử thế nào với các input đầu vào và đưa ra output.

Ta có thể coi mỗi layer chứa rất nhiều units hoạt động song song, đóng vai trò như một hàm ánh xạ vector sang giá trị số. Mặc dù các kiến trúc của mạng neural lấy rất nhiều ý tưởng từ ngành khoa học thần kinh, tuy nhiên, mục tiêu của mạng neural không phải để mô phỏng bộ não. Một cách nhìn tốt hơn là xem mạng neural như một máy học, được thiết kế với các điểm nhấn từ những gi chúng ta biết về não người.
Một cách để hiểu mạng neural là bắt đầu với các mô hình linear, và xem xét các hạn chế của các mô hình này. Linear models, như logistic regression hay linear regression, hấp dẫn ở chỗ chúng có thể fit rất hữu quả và đang tin cậy, với các close form và tối ưu hóa lồi. Một hàn chế hiển nhiên của linear là các hàm linear, nên model không thể hiểu tương tác giữa các input.
Để mở rộng lienar model để biểu diễn hàm nonlinear của x, chúng ta có thể áp dụng linear model không phải cho x mà cho (x), trong đó là một nonlinear transformer. Có thể xem như một tập các feature mô tả x, hoặc một cách để biểu diễn x. Vấn đề là chọn mapping . Có 3 cách thông dụng, chọn một generic như RBF, thiết kế bằng tay, hoặc học .

Nội dung chương này:

\begin{itemize}
  \item 1. Ví dụ cơ bản củ feedforward network
  \item 2. Design decisions cho việc thiết kế mạng feedforward network
  \item 3. Choose activation functions trong hidden layer
  \item 4. Thiết kế mạng neural, bao nhiêu layers, các layer kết nối với nhau như thế nào, có bao nhiêu unit trong một layer
  \begin{itemize}
    \item chosing the optimizer
    \item cost function
    \item form of the output units
  \end{itemize}
  \item 5. Thuật toán Back-propagation
  \item 6. Góc nhìn lịch sử
\end{itemize}


Tham khảo
Chapter 6, Deep Learning Book
Neural Network Zoo http://www.asimovinstitute.org/neural-network-zoo/


word2vec Example
Step 1: Download word2vec example from github

\begin{lstlisting}
> dir
02/06/2017 11:45 DIR .
02/06/2017 11:45 DIR ..
02/06/2017 10:12 9,144 word2vec_basic.py
\end{lstlisting}



Step 2: Run word2vec_basic example

\begin{lstlisting}
activate tensorflow-gpu
python word2vec_basic.py

Found and verified text8.zip
Data size 17005207
Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
Sample data [5241, 3082, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse',
'first', 'used', 'against']
3082 originated -> 5241 anarchism
3082 originated -> 12 as
12 as -> 6 a
12 as -> 3082 originated
6 a -> 195 term
6 a -> 12 as
195 term -> 2 of
195 term -> 6 a
Initialized
Average loss at step 0 : 288.173675537
Nearest to its: nasl, tinkering, derivational, yachts, emigrated, fatalism, kingston, kochi,
Nearest to into: streetcars, neglecting, deutschlands, lecture, realignment, bligh, donau, medalists,
Nearest to state: canterbury, exceptions, disaffection, crete, westernmost, earthly, organize, richland,

\end{lstlisting}



\section{taif}

\section{Tài liệu Deep Learning}

Lang thang thế nào lại thấy trang này \href{https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/DL_Reading_List.pdf}{My Reading List for Deep Learning!} của một anh ở Microsoft. Trong đó, (đương nhiên) có Deep Learning của thánh Yoshua Bengio, có một vụ hay nữa là bài review "Deep Learning" của mấy thánh Yann Lecun, Yoshua Bengio, Geoffrey Hinton trên tạp chí Nature. Ngoài ra còn có nhiều tài liệu hữu ích khác.

\section{Các layer trong deep learning}

\subsection{Sparse Layers}

\href{http://pytorch.org/docs/master/nn.html#embedding}{nn.Embedding} (\href{http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html}{hướng dẫn})

★ grep code: \href{https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py#L18}{Shawn1993/cnn-text-classification-pytorch}

Đóng vai trò như một lookup table, map một word với dense vector tương ứng

\subsection{Convolution Layers}

\index{convolution}

\href{http://pytorch.org/docs/master/nn.html#conv1d}{nn.Conv1d}, \href{http://pytorch.org/docs/master/nn.html#conv2d}{nn.Conv2d}, \href{http://pytorch.org/docs/master/nn.html#conv3d}{nn.Conv3d})

★ grep code: \href{https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py#L20-L24}{Shawn1993/cnn-text-classification-pytorch}, \href{https://github.com/galsang/CNN-sentence-classification-pytorch/blob/master/model.py#L36-L38}{galsang/CNN-sentence-classification-pytorch}

Các tham số trong Convolution Layer

* \textit{kernel\_size} (hay là filter size)

Đối với NLP, $kernel\_size$ thường bằng $region\_size * word\_dim$ (đối với conv1d) hay ($region\_size$, $word\_dim$) đối với conv2d

<small>Quá trình tạo feature map đối với region size bằng 2</small>
![](https://media.giphy.com/media/l2QE2y1UQP7vIgiti/giphy.gif)

* `in_channels`, `out_channels` (là số lượng `feature maps`)

Kênh (channels) là các cách nhìn (view) khác nhau đối với dữ liệu. Ví dụ, trong ảnh thường có 3 kênh RGB (red, green, blue), có thể áp dụng convolution giữa các kênh. Với văn bản cũng có thể có các kênh khác nhau, như khi có các kênh sử dụng các word embedding khác nhau (word2vec, GloVe), hoặc cùng một câu nhưng biểu diễn ở các ngôn ngữ khác nhau.

* `stride`

Định nghĩa bước nhảy của filter.

![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-05-at-10.18.08-AM-1024x251.png)

Hình minh họa sự khác biệt giữa các feature map đối với stride=1 và stride=2. Feature map đối với stride = 1 có kích thước là 5, feature map đối với stride = 3 có kích thước là 3. Stride càng lớn thì kích thước của feature map càng nhỏ.

Trong bài báo của Kim 2014, `stride = 1` đối với `nn.conv2d` và `stride = word_dim` đối với `nn.conv1d`

Toàn bộ tham số của mạng CNN trong bài báo Kim 2014,

![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png)

| Description | Values |
|---------------------|-----------------|
| input word vectors | Google word2vec |
| filter region size | (3, 4, 5)       |
| feature maps | 100 |
| activation function | ReLU |
| pooling | 1-max pooling |
| dropout rate | 0.5 |
| $latex l&amp;s=2$2 norm constraint | 3 |

Đọc thêm:

* [Lecture 13: Convolutional Neural Networks (for NLP). CS224n-2017](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture13-CNNs.pdf)
* [DeepNLP-models-Pytorch - 8. Convolutional Neural Networks](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/08.CNN-for-Text-Classification.ipynb)
* [A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification. Zhang 2015](https://arxiv.org/pdf/1510.03820.pdf)

\section{Recurrent Neural Networks}

\subsection{What are RNNs?}

The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps (more on this later). Here is what a typical RNN looks like:


A recurrent neural network and the unfolding in time of the computation involved in its forward computation

A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature The above diagram shows a RNN being unrolled (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. The formulas that govern the computation happening in a RNN are as follows:

xtxt is the input at time step tt. For example, x1x1 could be a one-hot vector corresponding to the second word of a sentence.
stst is the hidden state at time step tt. It’s the “memory” of the network. stst is calculated based on the previous hidden state and the input at the current step: st=f(Uxt+Wst−1)st=f(Uxt+Wst−1). The function ff usually is a nonlinearity such as tanh or ReLU. s−1s−1, which is required to calculate the first hidden state, is typically initialized to all zeroes.
otot is the output at step tt. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. ot=softmax(Vst)ot=softmax(Vst).
There are a few things to note here:

You can think of the hidden state stst as the memory of the network. stst captures information about what happened in all the previous time steps. The output at step otot is calculated solely based on the memory at time tt. As briefly mentioned above, it’s a bit more complicated in practice because stst typically can’t capture information from too many time steps ago.
Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (UU, VV, WW above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.
The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.

\subsection{What can RNNs do?}

RNNs have shown great success in many NLP tasks. At this point I should mention that the most commonly used type of RNNs are LSTMs, which are much better at capturing long-term dependencies than vanilla RNNs are. But don’t worry, LSTMs are essentially the same thing as the RNN we will develop in this tutorial, they just have a different way of computing the hidden state. We’ll cover LSTMs in more detail in a later post. Here are some example applications of RNNs in NLP (by non means an exhaustive list).

Language Modeling and Generating Text
Given a sequence of words we want to predict the probability of each word given the previous words. Language Models allow us to measure how likely a sentence is, which is an important input for Machine Translation (since high-probability sentences are typically correct). A side-effect of being able to predict the next word is that we get a generative model, which allows us to generate new text by sampling from the output probabilities. And depending on what our training data is we can generate all kinds of stuff. In Language Modeling our input is typically a sequence of words (encoded as one-hot vectors for example), and our output is the sequence of predicted words. When training the network we set ot=xt+1ot=xt+1 since we want the output at step tt to be the actual next word.

Research papers about Language Modeling and Generating Text:

Recurrent neural network based language model
Extensions of Recurrent neural network based language model
Generating Text with Recurrent Neural Networks
Machine Translation
Machine Translation is similar to language modeling in that our input is a sequence of words in our source language (e.g. German). We want to output a sequence of words in our target language (e.g. English). A key difference is that our output only starts after we have seen the complete input, because the first word of our translated sentences may require information captured from the complete input sequence.

RNN for Machine Translation


RNN for Machine Translation. Image Source: http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf

Research papers about Machine Translation:

A Recursive Recurrent Neural Network for Statistical Machine Translation
Sequence to Sequence Learning with Neural Networks
Joint Language and Translation Modeling with Recurrent Neural Networks
Speech Recognition
Given an input sequence of acoustic signals from a sound wave, we can predict a sequence of phonetic segments together with their probabilities.

Research papers about Speech Recognition:

Towards End-to-End Speech Recognition with Recurrent Neural Networks
Generating Image Descriptions
Together with convolutional Neural Networks, RNNs have been used as part of a model to generate descriptions for unlabeled images. It’s quite amazing how well this seems to work. The combined model even aligns the generated words with features found in the images.


Deep Visual-Semantic Alignments for Generating Image Descriptions. Source: http://cs.stanford.edu/people/karpathy/deepimagesent/

\subsection{Training RNNs}

Training a RNN is similar to training a traditional Neural Network. We also use the backpropagation algorithm, but with a little twist. Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. For example, in order to calculate the gradient at t=4t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time (BPTT). If this doesn’t make a whole lot of sense yet, don’t worry, we’ll have a whole post on the gory details. For now, just be aware of the fact that vanilla RNNs trained with BPTT have difficulties learning long-term dependencies (e.g. dependencies between steps that are far apart) due to what is called the vanishing/exploding gradient problem. There exists some machinery to deal with these problems, and certain types of RNNs (like LSTMs) were specifically designed to get around them.

RNN Extensions
Over the years researchers have developed more sophisticated types of RNNs to deal with some of the shortcomings of the vanilla RNN model. We will cover them in more detail in a later post, but I want this section to serve as a brief overview so that you are familiar with the taxonomy of models.

Bidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you want to look at both the left and the right context. Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs.

Deep (Bidirectional) RNNs are similar to Bidirectional RNNs, only that we now have multiple layers per time step. In practice this gives us a higher learning capacity (but we also need a lot of training data).

Deep Bidirectional RNN LSTM networks are quite popular these days and we briefly talked about them above. LSTMs don’t have a fundamentally different architecture from RNNs, but they use a different function to compute the hidden state. The memory in LSTMs are called cells and you can think of them as black boxes that take as input the previous state ht−1ht−1 and current input xtxt. Internally these cells decide what to keep in (and what to erase from) memory. They then combine the previous state, the current memory, and the input. It turns out that these types of units are very efficient at capturing long-term dependencies. LSTMs can be quite confusing in the beginning but if you’re interested in learning more this post has an excellent explanation.

Conclusion
So far so good. I hope you’ve gotten a basic understanding of what RNNs are and what they can do. In the next post we’ll implement a first version of our language model RNN using Python and Theano. Please leave questions in the comments!


[^1]: [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp)
[^2]: [http://pytorch.org/docs/master/nn.html](http://pytorch.org/docs/master/nn.html)

\section{Một số thư viện và framework hỗ trợ Deep Learning}

Cùng với sự phát triển của các thuật toán DL thì các thư viện cũng như framework hỗ trợ các thuật toán này cũng ngày càng tăng về số lượng. Hầu hết các thư viện và framework này đều cung cấp dưới dạng mã nguồn mở do đó rất linh hoạt trong việc sử dụng và mở rộng, đây cũng là một trong những lý do DL được áp dụng trong nhiều bài toán với nhiều lĩnh vực khác nhau. Trong phần tiếp theo nội dung post này sẽ giới thiệu một số thư viện phổ biến đang được cộng đồng nghiên cứu sử dụng.

framework

Caffe framework
Caffe được viết bằng C++ và phát triển bởi Yangqing Jia từ trung tâm Berkeley Vision Learning Center của đại học UC Berkeley. Được giới thiệu đầu tiên năm 2014 với mục đích là sử dụng để áp dụng thuật toán DL trong các bài toán liên quan đến thị giác máy, cho đến ngày nay cùng với sự đóng góp của cộng đồng, caffe framework được sử dụng cho nhiều lĩnh vực khác như xử lý ngôn ngữ tự nhiên, xử lý tiếng nói…

Sau đây là một số tính năng nổi bật của caffe framework:

Ngoài ngôn ngữ dùng để phát triển là C++, caffe framework còn hỗ trợ Matlab, Python.
Dễ dàng trong việc thiết lập và cài đặt do Caffe có kiến trúc đặc biệt trong đó sử dụng các thư viện đi kèm một cách độc lập.
Caffe cho phép người dùng có thể dẽ dàng tùy chọn huấn luyện thuật toán DL trên CPU hoặc GPU.
Người dùng có thể tự thiết lập một cấu trúc mạng cho hệ thống của mình theo một quy ước có trước, vì vậy người dùng không cần can thiệp quá sâu vào phần lập trình mà vẫn sử dụng được DL.
Người sử dụng có thể dễ dàng thực hiện quá trình huấn luyện trên dữ liệu của mình thông qua các câu lệnh đơn giản. Ngoài ra, người dùng cũng có thể sử dụng các mô hình đã được huấn luyện sẵn (pretrain model) do cộng đồng đóng góp (models zoo).
2. Torch

Torch được phát triển bằng ngôn ngữ Lua bởi nhóm nghiên cứu tại trường Đại học NewYork. Torch hiện được sử dụng rộng rãi trong nhiều phòng nghiên cứu cũng như các hãng công nghệ nổi tiếng Facebook, Google, Twitter, NYU, IDIAP …
Sau đây là một số tính năng nổi bật của Torch framework:

Torch được phát triển bằng ngôn ngữ Lua, một ngôn ngữ mà dễ dàng tích hợp với C. Do đó, chỉ trong vài giờ, bất kì thư viện C hay C++ nào cũng đều trở thành thư viện Lua.
Torch cũng có thể sử dụng các mô hình huấn luyện trước từ Caffe framework.
Torch có khả năng chạy trên nhiều hệ điều hành, trong đó bao gồm các hệ điều hành di động như iOS, Android.
Tuy nhiên Torch khi cần thiết kế hay sử dụng một cấu trúc mạng người dùng sẽ phải tự lập trình với các quy ước đã đặt ra trước.

3. TensorFlow

Thư viện TensorFlow viết bằng C++ và phát triển bởi Google và được giới thiệu vào tháng 11 năm 2015. Hiện tại TensorFlow được cho là sử dụng trong nhiều dịch vụ của Google như phân loại email của gmail, nhận biết phát âm và dịch tự động, nhận biết khuôn mặt trong Google Photo, tối ưu hoá kết quả tìm kiếm, quảng cáo trong Youtube, …

Một số tính năng nổi bật của Torch framework:

TensorFlow hỗ trợ cả hai ngôn ngữ c và python.
TensorFlow có thể chạy trên nhiều CPU cũng như GPU giúp đẩy nhanh quá trình huấn luyện cũng như xử lý dữ liệu thực từ mô mình đã được học. Ngoài ra với việc có thể sử dụng thư viện này trên các hệ thống cloud sẽ làm đẩy nhanh hiệu năng của các hệ thống sử dụng TensorFlow.
Với khả năng chạy trên nhiều hệ điều hành như bao gồm cả iOS, Android, hứa hẹn sẽ phát triển được các ứng dụng thông minh nhờ áp dụng các tính năng nổi bật của DL.
4. Theano

Theano là thư viện thuần Python được phát triển bởi LISA Lab – Đại Học Montreal. Theano cung cấp các thư viện tính toán, cho phép người dùng định nghĩa, tối ưu và đánh giá các hàm tính toán. Theano có khả năng xử lý một lượng lớn dữ liệu với thời gian nhanh chóng và cho phép sử dụng cả CPU và GPU.

Để thiết kế kiến trúc mạng neural cho riêng mình, người dùng phải tự lập trình từ những hàm của thư viện Theano.

Theano sử dụng 2 gói thư viện để hỗ trợ cho việc định nghĩa mô hình mạng neural:

Lasagne: Là thư viện định nghĩa các lớp (trừ lớp cuối cùng) của mô hình mạng neural. Lasagne giúp người dùng lưu trữ dữ liệu trong mạng, tính toán giá trị hàm lỗi, cập nhật trọng số.
Keras: Là lớp cuối cùng trong cấu trúc mạng. Hỗ trợ cài đặt các hàm kích hoạt và định nghĩa lớp softmax.
Tương tự như Caffe, Theano cũng có hỗ trợ các mô hình đã được huấn luyện do người sử dụng chia sẻ từ các trang web khác nhau.