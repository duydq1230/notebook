\chapter{X√°c su·∫•t}

\diary{19/01/2018 Nay t√¨m ƒë∆∞·ª£c kh√≥a \href{https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/ppt/}{CS 109} c·ªßa b√°c \href{https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/}{Chris Piech} qu√° hay}

X√°c su·∫•t l√† c√¥ng c·ª• ƒë·ªÉ m√¥ h√¨nh h√≥a c√°c s·ª± v·∫≠t t∆∞·ªüng nh∆∞ ng·∫´u nhi√™n.


\section{Probability Distributions}

When we use the word ‚Äúprobability‚Äù in day-to-day life, we refer to a degree of confidence that an event of an uncertain nature will occur. For example, the weather report might say ‚Äúthere is a low probability of light rain in the afternoon.‚Äù Probability theory deals with the formal foundations for discussing such estimates and the rules they should obey. Before we discuss the representation of probability, we need to define what the events are to which we want to assign a probability. These events might be different outcomes of throwing a die, the outcome of a horse race, the weather configurations in California, or the possible failures of a piece of machinery.

\subsection{Event Spaces}

event Formally, we define events by assuming that there is an agreed upon space of possible outcomes, outcome space which we denote by ‚Ñ¶. For example, if we consider dice, we might set ‚Ñ¶ = {1, 2, 3, 4, 5, 6}. In the case of a horse race, the space might be all possible orders of arrivals at the finish line, a much larger space.

measurable event In addition, we assume that there is a set of measurable events S to which we are willing to assign probabilities. Formally, each event Œ± ‚àà S is a subset of ‚Ñ¶. In our die example, the event {6} represents the case where the die shows 6, and the event {1, 3, 5} represents the case of an odd outcome. In the horse-race example, we might consider the event ‚ÄúLucky Strike wins,‚Äù which contains all the outcomes in which the horse Lucky Strike is first. Probability theory requires that the event space satisfy three basic properties: ‚Ä¢ It contains the empty event ‚àÖ, and the trivial event ‚Ñ¶. ‚Ä¢ It is closed under union. That is, if Œ±, Œ≤ ‚àà S, then so is Œ± ‚à™ Œ≤. ‚Ä¢ It is closed under complementation. That is, if Œ± ‚àà S, then so is ‚Ñ¶ ‚àí Œ±. The requirement that the event space is closed under union and complementation implies that it is also closed under other Boolean operations, such as intersection and set dierence.

\subsection{Probability Distributions}

Definition 2.1 A probability distribution P over (‚Ñ¶, S) is a mapping from events in S to real values that satisfies probability distribution the following conditions: ‚Ä¢ P(Œ±) ‚â• 0 for all Œ± ‚àà S. ‚Ä¢ P(‚Ñ¶) = 1. ‚Ä¢ If Œ±, Œ≤ ‚àà S and Œ± ‚à© Œ≤ = ‚àÖ, then P(Œ± ‚à™ Œ≤) = P(Œ±) + P(Œ≤). The first condition states that probabilities are not negative. The second states that the ‚Äútrivial event,‚Äù which allows all possible outcomes, has the maximal possible probability of 1. The third condition states that the probability that one of two mutually disjoint events will occur is the sum of the probabilities of each event. These two conditions imply many other conditions. Of particular interest are P(‚àÖ) = 0, and P(Œ± ‚à™ Œ≤) = P(Œ±) + P(Œ≤) ‚àí P(Œ± ‚à© Œ≤).


1.3 Interpretations of Probability
Before we continue to discuss probability distributions, we need to consider the interpretations that we might assign to them. Intuitively, the probability P(Œ±) of an event Œ± quantifies the degree of confidence that Œ± will occur. If P(Œ±) = 1, we are certain that one of the outcomes in Œ± occurs, and if P(Œ±) = 0, we consider all of them impossible. Other probability values represent options that lie between these two extremes. This description, however, does not provide an answer to what the numbers mean. There are two common interpretations for probabilities. frequentist The frequentist interpretation views probabilities as frequencies of events. More precisely, the interpretation probability of an event is the fraction of times the event occurs if we repeat the experiment indefinitely. For example, suppose we consider the outcome of a particular die roll. In this case, the statement P(Œ±) = 0.3, for Œ± = {1, 3, 5}, states that if we repeatedly roll this die and record the outcome, then the fraction of times the outcomes in Œ± will occur is 0.3. More precisely, the limit of the sequence of fractions of outcomes in Œ± in the first roll, the first two rolls, the first three rolls, . . ., the first n rolls, . . . is 0.3.

The frequentist interpretation gives probabilities a tangible semantics. When we discuss concrete physical systems (for example, dice, coin flips, and card games) we can envision how these frequencies are defined. It is also relatively straightforward to check that frequencies must satisfy the requirements of proper distributions. The frequentist interpretation fails, however, when we consider events such as ‚ÄúIt will rain tomorrow afternoon.‚Äù Although the time span of ‚ÄúTomorrow afternoon‚Äù is somewhat ill defined, we expect it to occur exactly once. It is not clear how we define the frequencies of such events. Several attempts have been made to define the probability for such an event by finding a reference class reference class of similar events for which frequencies are well defined; however, none of them has proved entirely satisfactory. Thus, the frequentist approach does not provide a satisfactory interpretation for a statement such as ‚Äúthe probability of rain tomorrow afternoon is 0.3.‚Äù  An alternative interpretation views probabilities as subjective degrees of belief. Under subjective interpretation this interpretation, the statement P(Œ±) = 0.3 represents a subjective statement about one‚Äôs own degree of belief that the event Œ± will come about. Thus, the statement ‚Äúthe probability of rain tomorrow afternoon is 50 percent‚Äù tells us that in the opinion of the speaker, the chances of rain and no rain tomorrow afternoon are the same. Although tomorrow afternoon will occur only once, we can still have uncertainty about its outcome, and represent it using numbers (that is, probabilities). This description still does not resolve what exactly it means to hold a particular degree of belief. What stops a person from stating that the probability that Bush will win the election is 0.6 and the probability that he will lose is 0.8? The source of the problem is that we need to explain how subjective degrees of beliefs (something that is internal to each one of us) are reflected in our actions. This issue is a major concern in subjective probabilities. One possible way of attributing degrees of beliefs is by a betting game. Suppose you believe that P(Œ±) = 0.8. Then you would be willing to place a bet of \$1 against \$3. To see this, note that with probability 0.8 you gain a dollar, and with probability 0.2 you lose \$3, so on average this bet is a good deal with expected gain of 20 cents. In fact, you might be even tempted to place a bet of \$1 against \$4. Under this bet the average gain is 0, so you should not mind. However, you would not consider it worthwhile to place a bet \$1 against

    4 and 10 cents, since that would have negative expected gain. Thus, by finding which bets you are willing to place, we can assess your degrees of beliefs. The key point of this mental game is the following. If you hold degrees of belief that do not satisfy the rule of probability, then by a clever construction we can find a series of bets that would result in a sure negative outcome for you. Thus, the argument goes, a rational person must hold degrees of belief that satisfy the rules of probability.1 In the remainder of the book we discuss probabilities, but we usually do not explicitly state their interpretation. Since both interpretations lead to the same mathematical rules, the technical definitions hold for both interpretations.

\section{Basic Concepts in Probability}

\subsection{Conditional Probability}

To use a concrete example, suppose we consider a distribution over a population of students taking a certain course. The space of outcomes is simply the set of all students in the population. Now, suppose that we want to reason about the students‚Äô intelligence and their final grade. We can define the event \alpha to denote ‚Äúall students with grade A,‚Äù and the event Œ≤ to denote ‚Äúall students with high intelligence.‚Äù Using our distribution, we can consider the probability of these events, as well as the probability of Œ± ‚à© Œ≤ (the set of intelligent students who got grade A). This, however, does not directly tell us how to update our beliefs given new evidence. Suppose we learn that a student has received the grade A; what does that tell us about her intelligence? This kind of question arises every time we want to use distributions to reason about the real world. More precisely, after learning that an event Œ± is true, how do we change our probability conditional about Œ≤ occurring? The answer is via the notion of conditional probability. Formally, the probability conditional probability of Œ≤ given Œ± is defined as P(Œ≤ | Œ±) = P(Œ± ‚à© Œ≤) P(Œ±) (2.1) That is, the probability that Œ≤ is true given that we know Œ± is the relative proportion of outcomes satisfying Œ≤ among these that satisfy Œ±. (Note that the conditional probability is not defined when P(Œ±) = 0.) The conditional probability given an event (say Œ±) satisfies the properties of definition 2.1 (see exercise 2.4), and thus it is a probability distribution by its own right. Hence, we can think of the conditioning operation as taking one distribution and returning another over the same probability space.

\section{Chain Rule and Bayes Rule}

From the definition of the conditional distribution, we immediately see that P(Œ± ‚à© Œ≤) = P(Œ±)P(Œ≤ | Œ±). (2.2) chain rule This equality is known as the chain rule of conditional probabilities. More generally, if Œ±1, . . . , Œ±k are events, then we can write P(Œ±1 ‚à© . . . ‚à© Œ±k) = P(Œ±1)P(Œ±2 | Œ±1) ¬∑ ¬∑ ¬∑ P(Œ±k | Œ±1 ‚à© . . . ‚à© Œ±k‚àí1). (2.3) In other words, we can express the probability of a combination of several events in terms of the probability of the first, the probability of the second given the first, and so on. It is important to notice that we can expand this expression using any order of events ‚Äî the result will remain the same. Bayes‚Äô rule Another immediate consequence of the definition of conditional probability is Bayes‚Äô rule P(Œ± | Œ≤) = P(Œ≤ | Œ±)P(Œ±) P(Œ≤)

A more general conditional version of Bayes‚Äô rule, where all our probabilities are conditioned on some background event Œ≥, also holds: P(Œ± | Œ≤ ‚à© Œ≥) = P(Œ≤ | Œ± ‚à© Œ≥)P(Œ± | Œ≥) P(Œ≤ | Œ≥) . Bayes‚Äô rule is important in that it allows us to compute the conditional probability P(Œ± | Œ≤) from the ‚Äúinverse‚Äù conditional probability P(Œ≤ | Œ±). Example 2.1 Consider the student population, and let Smart denote smart students and GradeA denote students who got grade A. Assume we believe (perhaps based on estimates from past statistics) that P(GradeA | Smart) = 0.6, and now we learn that a particular student received grade A. Can we estimate the probability that the student is smart? According to Bayes‚Äô rule, this depends on prior our prior probability for students being smart (before we learn anything about them) and the prior probability of students receiving high grades. For example, suppose that P(Smart) = 0.3 and P(GradeA) = 0.2, then we have that P(Smart | GradeA) = 0.6 ‚àó 0.3/0.2 = 0.9. That is, an A grade strongly suggests that the student is smart. On the other hand, if the test was easier and high grades were more common, say, P(GradeA) = 0.4 then we would get that P(Smart | GradeA) = 0.6 ‚àó 0.3/0.4 = 0.45, which is much less conclusive about the student. Another classic example that shows the importance of this reasoning is in disease screening. To see this, consider the following hypothetical example (none of the mentioned figures are related to real statistics). Example 2.2 Suppose that a tuberculosis (TB) skin test is 95 percent accurate. That is, if the patient is TB-infected, then the test will be positive with probability 0.95, and if the patient is not infected, then the test will be negative with probability 0.95. Now suppose that a person gets a positive test result. What is the probability that he is infected? Naive reasoning suggests that if the test result is wrong 5 percent of the time, then the probability that the subject is infected is 0.95. That is, 95 percent of subjects with positive results have TB. If we consider the problem by applying Bayes‚Äô rule, we see that we need to consider the prior probability of TB infection, and the probability of getting positive test result. Suppose that 1 in 1000 of the subjects who get tested is infected. That is, P(TB) = 0.001. What is the probability of getting a positive test result? From our description, we see that 0.001 ¬∑ 0.95 infected subjects get a positive result, and 0.999¬∑0.05 uninfected subjects get a positive result. Thus, P(Positive) = 0.0509. Applying Bayes‚Äô rule, we get that P(TB | Positive) = 0.001¬∑0.95/0.0509 ‚âà 0.0187. Thus, although a subject with a positive test is much more probable to be TB-infected than is a random subject, fewer than 2 percent of these subjects are TB-infected.

\section{Random Variables and Joint Distributions}

\subsection{Motivation}

Our discussion of probability distributions deals with events. Formally, we can consider any event from the set of measurable events. The description of events is in terms of sets of outcomes. In many cases, however, it would be more natural to consider attributes of the outcome. For example, if we consider a patient, we might consider attributes such as ‚Äúage,‚Äù

‚Äúgender,‚Äù and ‚Äúsmoking history‚Äù that are relevant for assigning probability over possible diseases and symptoms. We would like then consider events such as ‚Äúage > 55, heavy smoking history, and suers from repeated cough.‚Äù To use a concrete example, consider again a distribution over a population of students in a course. Suppose that we want to reason about the intelligence of students, their final grades, and so forth. We can use an event such as GradeA to denote the subset of students that received the grade A and use it in our formulation. However, this discussion becomes rather cumbersome if we also want to consider students with grade B, students with grade C, and so on. Instead, we would like to consider a way of directly referring to a student‚Äôs grade in a clean, mathematical way. The formal machinery for discussing attributes and their values in dierent outcomes are random variable random variables. A random variable is a way of reporting an attribute of the outcome. For example, suppose we have a random variable Grade that reports the final grade of a student, then the statement P (Grade = A) is another notation for P (GradeA).

n the statement P (Grade = A) is another notation for P (GradeA).

\subsection{What Is a Random Variable?}

Formally, a random variable, such as Grade, is defined by a function that associates with each outcome in ‚Ñ¶ a value. For example, Grade is defined by a function fGrade that maps each person in ‚Ñ¶ to his or her grade (say, one of A, B, or C). The event Grade = A is a shorthand for the event {œâ ‚àà ‚Ñ¶ : fGrade(œâ) = A}. In our example, we might also have a random variable Intelligence that (for simplicity) takes as values either ‚Äúhigh‚Äù or ‚Äúlow.‚Äù In this case, the event ‚ÄúIntelligence = high‚Äù refers, as can be expected, to the set of smart (high intelligence) students. Random variables can take dierent sets of values. We can think of categorical (or discrete) random variables that take one of a few values, as in our two preceding examples. We can also talk about random variables that can take infinitely many values (for example, integer or real values), such as Height that denotes a student‚Äôs height. We use Val(X) to denote the set of values that a random variable X can take. In most of the discussion in this book we examine either categorical random variables or random variables that take real values. We will usually use uppercase roman letters X, Y, Z to denote random variables. In discussing generic random variables, we often use a lowercase letter to refer to a value of a random variable. Thus, we use x to refer to a generic value of X. For example, in statements such as ‚ÄúP (X = x) ‚â• 0 for all x ‚àà Val(X).‚Äù When we discuss categorical random variables, we use the notation x1, . . . , xk, for k = |Val(X)| (the number of elements in Val(X)), when we need to enumerate the specific values of X, for example, in statements such as kX i =1 P (X = xi) = 1. multinomial The distribution over such a variable is called a multinomial. In the case of a binary-valued distribution random variable X, where Val(X) = {false, true}, we often use x1 to denote the value true for X, and x0 to denote the value false. The distribution of such a random variable is called a Bernoulli Bernoulli distribution. distribution We also use boldface type to denote sets of random variables. Thus, X, Y , or Z are typically used to denote a set of random variables, while x, y, z denote assignments of values to the

variables in these sets. We extend the definition of Val(X) to refer to sets of variables in the obvious way. Thus, x is always a member of Val(X). For Y ‚äÜ X, we use xhY i to refer to the assignment within x to the variables in Y . For two assignments x (to X) and y (to Y ), we say that x ‚àº y if they agree on the variables in their intersection, that is, xhX ‚à© Y i = yhX ‚à© Y i. In many cases, the notation P(X = x) is redundant, since the fact that x is a value of X is already reported by our choice of letter. Thus, in many texts on probability, the identity of a random variable is not explicitly mentioned, but can be inferred through the notation used for its value. Thus, we use P(x) as a shorthand for P(X = x) when the identity of the random variable is clear from the context. Another shorthand notation is that Px refers to a sum over all possible values that X can take. Thus, the preceding statement will often appear as Px P(x) = 1. Finally, another standard notation has to do with conjunction. Rather than write P((X = x) ‚à© (Y = y)), we write P(X = x, Y = y), or just P(x, y).

3.3 Marginal and Joint Distributions
Once we define a random variable X, we can consider the distribution over events that can be marginal described using X. This distribution is often referred to as the marginal distribution over the distribution random variable X. We denote this distribution by P(X). Returning to our population example, consider the random variable Intelligence. The marginal distribution over Intelligence assigns probability to specific events such as P(Intelligence = high) and P(Intelligence = low), as well as to the trivial event P(Intelligence ‚àà {high, low}). Note that these probabilities are defined by the probability distribution over the original space. For concreteness, suppose that P(Intelligence = high) = 0.3, P(Intelligence = low) = 0.7. If we consider the random variable Grade, we can also define a marginal distribution. This is a distribution over all events that can be described in terms of the Grade variable. In our example, we have that P(Grade = A) = 0.25, P(Grade = B) = 0.37, and P(Grade = C) = 0.38. It should be fairly obvious that the marginal distribution is a probability distribution satisfying the properties of definition 2.1. In fact, the only change is that we restrict our attention to the subsets of S that can be described with the random variable X. In many situations, we are interested in questions that involve the values of several random variables. For example, we might be interested in the event ‚ÄúIntelligence = high and Grade = A.‚Äù joint distribution To discuss such events, we need to consider the joint distribution over these two random variables. In general, the joint distribution over a set X = {X1, . . . , Xn} of random variables is denoted by P(X1, . . . , Xn) and is the distribution that assigns probabilities to events that are specified in terms of these random variables. We use Œæ to refer to a full assignment to the variables in X , that is, Œæ ‚àà Val(X). The joint distribution of two random variables has to be consistent with the marginal distribution, in that P(x) = P y P(x, y). This relationship is shown in figure 2.1, where we compute the marginal distribution over Grade by summing the probabilities along each row. Similarly, we find the marginal distribution over Intelligence by summing out along each column. The resulting sums are typically written in the row or column margins, whence the term ‚Äúmarginal distribution.‚Äù Suppose we have a joint distribution over the variables X = {X1, . . . , Xn}. The most fine-grained events we can discuss using these variables are ones of the form ‚ÄúX1 = x1 and X2 = x2, . . ., and Xn = xn‚Äù for a choice of values x1, . . . , xn for all the variables. Moreover,

Intelligence low high A 0.07 0.18 0.25 Grade B 0.28 0.09 0.37 C 0.35 0.03 0.38 0.7 0.3 1 Figure 2.1 Example of a joint distribution P(Intelligence, Grade): Values of Intelligence (columns) and Grade (rows) with the associated marginal distribution on each variable. any two such events must be either identical or disjoint, since they both assign values to all the variables in X . In addition, any event defined using variables in X must be a union of a set of canonical such events. Thus, we are eectively working in a canonical outcome space: a space where each outcome space outcome corresponds to a joint assignment to X1, . . . , Xn. More precisely, all our probability computations remain the same whether we consider the original outcome space (for example, all students), or the canonical space (for example, all combinations of intelligence and grade). atomic outcome We use Œæ to denote these atomic outcomes: those assigning a value to each variable in X . For example, if we let X = {Intelligence, Grade}, there are six atomic outcomes, shown in figure 2.1. The figure also shows one possible joint distribution over these six outcomes. Based on this discussion, from now on we will not explicitly specify the set of outcomes and measurable events, and instead implicitly assume the canonical outcome space.

3.4 Conditional Probability
The notion of conditional probability extends to induced distributions over random variables. For conditional example, we use the notation P (Intelligence | Grade = A) to denote the conditional distribution distribution over the events describable by Intelligence given the knowledge that the student‚Äôs grade is A. Note that the conditional distribution over a random variable given an observation of the value of another one is not the same as the marginal distribution. In our example, P (Intelligence = high) = 0.3, and P (Intelligence = high | Grade = A) = 0.18/0.25 = 0.72. Thus, clearly P (Intelligence | Grade = A) is dierent from the marginal distribution P (Intelligence). The latter distribution represents our prior knowledge about students before learning anything else about a particular student, while the conditional distribution represents our more informed distribution after learning her grade. We will often use the notation P (X | Y ) to represent a set of conditional probability distributions. Intuitively, for each value of Y , this object assigns a probability over values of X using the conditional probability. This notation allows us to write the shorthand version of the chain rule: P (X, Y ) = P (X)P (Y | X), which can be extended to multiple variables as P (X1, . . . , Xk) = P (X1)P (X2 | X1) ¬∑ ¬∑ ¬∑ P (Xk | X1, . . . , Xk‚àí1). (2.5) Similarly, we can state Bayes‚Äô rule in terms of conditional probability distributions: P (X | Y ) = P (X)P (Y | X) P (Y ) . (2.6)

\section{Independence and Conditional Independence}

4.1 Independence
As we mentioned, we usually expect P(Œ±|Œ≤)P(Œ±|Œ≤) to be different from P(Œ±)P(Œ±). That is, learning that Œ≤Œ≤ is true changes our probability over Œ±Œ±. However, in some situations equality can occur, so that P(Œ±|Œ≤)=P(Œ±)P(Œ±|Œ≤)=P(Œ±). That is, learning that Œ≤Œ≤ occurs did not change our probability of Œ±Œ±.

Definition independent events

We say that an event Œ±Œ± is independent of event Œ≤Œ≤ in PP, denoted P‚ä®(Œ±‚ä•Œ≤)P‚ä®(Œ±‚ä•Œ≤), if P(Œ±|Œ≤)=P(Œ±)P(Œ±|Œ≤)=P(Œ±) or if P(Œ≤)=0P(Œ≤)=0.

We can also provide an alternative definition for the concept of independence:

Proposition 2.1

A distribution PP satisfies (Œ±‚ä•Œ≤)(Œ±‚ä•Œ≤) if and only if P(Œ±‚à©Œ≤)=P(Œ±)P(Œ≤)P(Œ±‚à©Œ≤)=P(Œ±)P(Œ≤).

PROOF Consider first the case where P(Œ≤)=0P(Œ≤)=0; here, we also have P(Œ±‚à©Œ≤)=0P(Œ±‚à©Œ≤)=0, and so the equivalence immediately holds. When P(Œ≤)‚â†0P(Œ≤)‚â†0, we can use the chain rule; we write P(Œ±‚à©Œ≤)=P(Œ±|Œ≤)P(Œ≤)P(Œ±‚à©Œ≤)=P(Œ±|Œ≤)P(Œ≤). Since Œ±Œ± is independent of Œ≤Œ≤, we have that P(Œ±|Œ≤)=P(Œ±)P(Œ±|Œ≤)=P(Œ±). Thus, P(Œ±‚à©Œ≤)=P(Œ±)P(Œ≤)P(Œ±‚à©Œ≤)=P(Œ±)P(Œ≤). Conversely, suppose that P(Œ±‚à©Œ≤)=P(Œ±)P(Œ≤)P(Œ±‚à©Œ≤)=P(Œ±)P(Œ≤). Then, by definition, we have that

P(Œ±|Œ≤)=P(Œ±‚à©Œ≤)P(Œ≤)=P(Œ±)P(Œ≤)P(Œ≤)=P(Œ±).
P(Œ±|Œ≤)=P(Œ±‚à©Œ≤)P(Œ≤)=P(Œ±)P(Œ≤)P(Œ≤)=P(Œ±).
As an immediate consequence of this alternative definition, we see that independence is a symmetric notion. That is, (Œ± ‚ä• Œ≤) implies (Œ≤ ‚ä• Œ±). Example 2.3 For example, suppose that we toss two coins, and let Œ± be the event ‚Äúthe first toss results in a head‚Äù and Œ≤ the event ‚Äúthe second toss results in a head.‚Äù It is not hard to convince ourselves that we expect that these two events to be independent. Learning that Œ≤ is true would not change our probability of Œ±. In this case, we see two dierent physical processes (that is, coin tosses) leading to the events, which makes it intuitive that the probabilities of the two are independent. In certain cases, the same process can lead to independent events. For example, consider the event Œ± denoting ‚Äúthe die outcome is even‚Äù and the event Œ≤ denoting ‚Äúthe die outcome is 1 or 2.‚Äù It is easy to check that if the die is fair (each of the six possible outcomes has probability 1 6 ), then these two events are independent.

4.2 Conditional Independence
 While independence is a useful property, it is not often that we encounter two indepen- dent events. A more common situation is when two events are independent given an additional event. For example, suppose we want to reason about the chance that our student is accepted to graduate studies at Stanford or MIT. Denote by Stanford the event ‚Äúadmitted to Stanford‚Äù and by MIT the event ‚Äúadmitted to MIT.‚Äù In most reasonable distributions, these two events are not independent. If we learn that a student was admitted to Stanford, then our estimate of her probability of being accepted at MIT is now higher, since it is a sign that she is a promising student

Now, suppose that both universities base their decisions only on the student‚Äôs grade point average (GPA), and we know that our student has a GPA of A. In this case, we might argue that learning that the student was admitted to Stanford should not change the probability that she will be admitted to MIT: Her GPA already tells us the information relevant to her chances of admission to MIT, and finding out about her admission to Stanford does not change that. Formally, the statement is P(MIT | Stanford, GradeA) = P(MIT | GradeA). In this case, we say that MIT is conditionally independent of Stanford given GradeA. Definition 2.3 We say that an event Œ± is conditionally independent of event Œ≤ given event Œ≥ in P, denoted conditional independence P |= (Œ± ‚ä• Œ≤ | Œ≥), if P(Œ± | Œ≤ ‚à© Œ≥) = P(Œ± | Œ≥) or if P(Œ≤ ‚à© Œ≥) = 0. It is easy to extend the arguments we have seen in the case of (unconditional) independencies to give an alternative definition. Proposition 2.2 P satisfies (Œ± ‚ä• Œ≤ | Œ≥) if and only if P(Œ± ‚à© Œ≤ | Œ≥) = P(Œ± | Œ≥)P(Œ≤ | Œ≥).

4.3 Independence of Random Variables
Until now, we have focused on independence between events. Thus, we can say that two events, such as one toss landing heads and a second also landing heads, are independent. However, we would like to say that any pair of outcomes of the coin tosses is independent. To capture such statements, we can examine the generalization of independence to sets of random variables. Definition 2.4 Let X, Y , Z be sets of random variables. We say that X is conditionally independent of Y given conditional independence Z in a distribution P if P satisfies (X = x ‚ä• Y = y | Z = z) for all values x ‚àà Val(X), y ‚àà Val(Y ), and z ‚àà Val(Z). The variables in the set Z are often said to be observed. If the set observed variable Z is empty, then instead of writing (X ‚ä• Y | ‚àÖ), we write (X ‚ä• Y ) and say that X and Y are marginally independent. marginal independence Thus, an independence statement over random variables is a universal quantification over all possible values of the random variables. The alternative characterization of conditional independence follows immediately: Proposition 2.3 The distribution P satisfies (X ‚ä• Y | Z) if and only if P(X, Y | Z) = P(X | Z)P(Y | Z). Suppose we learn about a conditional independence. Can we conclude other independence properties that must hold in the distribution? We have already seen one such example: symmetry ‚Ä¢ Symmetry: (X ‚ä• Y | Z) =‚áí (Y ‚ä• X | Z). (2.7) There are several other properties that hold for conditional independence, and that often provide a very clean method for proving important properties about distributions. Some key properties are:

‚Ä¢ Decomposition: (X ‚ä• Y , W | Z) =‚áí (X ‚ä• Y | Z). (2.8) weak union ‚Ä¢ Weak union: (X ‚ä• Y , W | Z) =‚áí (X ‚ä• Y | Z, W). (2.9) contraction ‚Ä¢ Contraction: (X ‚ä• W | Z, Y )&(X ‚ä• Y | Z) =‚áí (X ‚ä• Y , W | Z). (2.10) An additional important property does not hold in general, but it does hold in an important subclass of distributions. Definition 2.5 A distribution P is said to be positive if for all events Œ± ‚àà S such that Œ± 6= ‚àÖ, we have that positive distribution P(Œ±) > 0. For positive distributions, we also have the following property: intersection ‚Ä¢ Intersection: For positive distributions, and for mutually disjoint sets X, Y , Z, W : (X ‚ä• Y | Z, W)&(X ‚ä• W | Z, Y ) =‚áí (X ‚ä• Y , W | Z). (2.11) The proof of these properties is not dicult. For example, to prove Decomposition, assume that (X ‚ä• Y, W | Z) holds. Then, from the definition of conditional independence, we have that P(X, Y, W | Z) = P(X | Z)P(Y, W | Z). Now, using basic rules of probability and arithmetic, we can show P(X, Y | Z) = X w P(X, Y, w | Z) = X w P(X | Z)P(Y, w | Z) = P(X | Z) X w P(Y, w | Z) = P(X | Z)P(Y | Z). The only property we used here is called ‚Äúreasoning by cases‚Äù (see exercise 2.6). We conclude that (X ‚ä• Y | Z).

\section{Querying a Distribution}

Our focus throughout this book is on using a joint probability distribution over multiple random variables to answer queries of interest.

\subsection{5.1 Probability Queries}

probability query Perhaps the most common query type is the probability query. Such a query consists of two parts: evidence ‚Ä¢ The evidence: a subset E of random variables in the model, and an instantiation e to these variables; query variables ‚Ä¢ the query variables: a subset Y of random variables in the network. Our task is to compute P(Y | E = e), posterior that is, the posterior probability distribution over the values y of Y , conditioned on the fact that distribution E = e. This expression can also be viewed as the marginal over Y , in the distribution we obtain by conditioning on e.

\subsection{5.2 MAP Queries}

A second important type of task is that of finding a high-probability joint assignment to some subset of variables. The simplest variant of this type of task is the MAP query (also called MAP assignment most probable explanation (MPE)), whose aim is to find the MAP assignment ‚Äî the most likely assignment to all of the (non-evidence) variables. More precisely, if we let W = X ‚àí E, our task is to find the most likely assignment to the variables in W given the evidence E = e: MAP(W | e) = argmax w P(w, e), (2.12) where, in general, argmaxx f(x) represents the value of x for which f(x) is maximal. Note that there might be more than one assignment that has the highest posterior probability. In this case, we can either decide that the MAP task is to return the set of possible assignments, or to return an arbitrary member of that set. It is important to understand the dierence between MAP queries and probability queries. In a MAP query, we are finding the most likely joint assignment to W . To find the most likely assignment to a single variable A, we could simply compute P(A | e) and then pick the most likely value. However, the assignment where each variable individually picks its most  likely value can be quite dierent from the most likely joint assignment to all variables simultaneously. This phenomenon can occur even in the simplest case, where we have no evidence. Example 2.4 Consider a two node chain A ‚Üí B where A and B are both binary-valued. Assume that: a0 a1 0.4 0.6 A b0 b1 a0 0.1 0.9 a1 0.5 0.5 (2.13) We can see that P(a1) > P(a0), so that MAP(A) = a1. However, MAP(A, B) = (a0, b1): Both values of B have the same probability given a1. Thus, the most likely assignment containing a1 has probability 0.6 √ó 0.5 = 0.3. On the other hand, the distribution over values of B is more skewed given a0, and the most likely assignment (a0, b1) has the probability 0.4 √ó 0.9 = 0.36. Thus, we have that argmaxa,b P(a, b) 6= (argmaxa P(a),argmaxb P(b)).

\subsection{5.3 Marginal MAP Queries}

To motivate our second query type, let us return to the phenomenon demonstrated in example 2.4. Now, consider a medical diagnosis problem, where the most likely disease has multiple possible symptoms, each of which occurs with some probability, but not an overwhelming probability. On the other hand, a somewhat rarer disease might have only a few symptoms, each of which is very likely given the disease. As in our simple example, the MAP assignment to the data and the symptoms might be higher for the second disease than for the first one. The solution here is to look for the most likely assignment to the disease variable(s) only, rather than the most likely assignment to both the disease and symptom variables. This approach suggests marginal MAP the use of a more general query type. In the marginal MAP query, we have a subset of variables Y that forms our query. The task is to find the most likely assignment to the variables in Y given the evidence E = e: MAP(Y | e) = arg max y P(y | e). If we let Z = X ‚àí Y ‚àí E, the marginal MAP task is to compute: MAP(Y | e) = arg max Y X Z P(Y , Z | e). Thus, marginal MAP queries contain both summations and maximizations; in a way, it contains elements of both a conditional probability query and a MAP query. Note that example 2.4 shows that marginal MAP assignments are not monotonic: the most likely assignment MAP(Y1 | e) might be completely dierent from the assignment to Y1 in MAP({Y1, Y2} | e). Thus, in particular, we cannot use a MAP query to give us the correct answer to a marginal MAP query.

\section{Continuous Spaces}
In the previous section, we focused on random variables that have a finite set of possible values. In many situations, we also want to reason about continuous quantities such as weight, height, duration, or cost that take real numbers in IR. When dealing with probabilities over continuous random variables, we have to deal with some technical issues. For example, suppose that we want to reason about a random variable X that can take values in the range between 0 and 1. That is, Val(X) is the interval [0, 1]. Moreover, assume that we want to assign each number in this range equal probability. What would be the probability of a number x? Clearly, since each x has the same probability, and there are infinite number of values, we must have that P(X = x) = 0. This problem appears even if we do not require uniform probability.

6.1 Probability Density Functions
How do we define probability over a continuous random variable? We say that a function density function p : IR 7‚Üí IR is a probability density function or (PDF) for X if it is a nonnegative integrable

function such that Z Val(X) p(x)dx = 1. That is, the integral over the set of possible values of X is 1. The PDF defines a distribution for X as follows: for any x in our event space: P(X ‚â§ a) = aZ ‚àí‚àû p(x)dx. cumulative The function P is the cumulative distribution for X. We can easily employ the rules of distribution probability to see that by using the density function we can evaluate the probability of other events. For example, P(a ‚â§ X ‚â§ b) = bZa p(x)dx. Intuitively, the value of a PDF p(x) at a point x is the incremental amount that x adds to the cumulative distribution in the integration process. The higher the value of p at and around x, the more mass is added to the cumulative distribution as it passes x. The simplest PDF is the uniform distribution. Definition 2.6 A variable X has a uniform distribution over [a, b], denoted X ‚àº Unif[a,b] if it has the PDF uniform distribution p(x) =  0 otherwise b‚àí 1a b ‚â• x ‚â• a. Thus, the probability of any subinterval of [a, b] is proportional its size relative to the size of [a, b]. Note that, if b ‚àí a < 1, then the density can be greater than 1. Although this looks unintuitive, this situation can occur even in a legal PDF, if the interval over which the value is greater than 1 is not too large. We have only to satisfy the constraint that the total area under the PDF is 1. As a more complex example, consider the Gaussian distribution. Definition 2.7 A random variable X has a Gaussian distribution with mean ¬µ and variance œÉ2, denoted X ‚àº Gaussian distribution N ¬µ; œÉ2, if it has the PDF p(x) = ‚àö21œÄœÉ e‚àí (x2 ‚àí œÉ¬µ 2)2 . standard A standard Gaussian is one with mean 0 and variance 1. Gaussian A Gaussian distribution has a bell-like curve, where the mean parameter ¬µ controls the location of the peak, that is, the value for which the Gaussian gets its maximum value. The variance parameter œÉ2 determines how peaked the Gaussian is: the smaller the variance, the


more peaked the Gaussian. Figure 2.2 shows the probability density function of a few dierent Gaussian distributions. More technically, the probability density function is specified as an exponential, where the expression in the exponent corresponds to the square of the number of standard deviations œÉ that x is away from the mean ¬µ. The probability of x decreases exponentially with the square of its deviation from the mean, as measured in units of its standard deviation.

6.2 Joint Density Functions

The discussion of density functions for a single variable naturally extends for joint distributions of continuous random variables. Definition 2.8 Let P be a joint distribution over continuous random variables X1, . . . , Xn. A function p(x1, . . . , xn) joint density is a joint density function of X1, . . . , Xn if ‚Ä¢ p(x1, . . . , xn) ‚â• 0 for all values x1, . . . , xn of X1, . . . , Xn. ‚Ä¢ p is an integrable function. ‚Ä¢ For any choice of a1, . . . , an, and b1, . . . , bn, P(a1 ‚â§ X1 ‚â§ b1, . . . , an ‚â§ Xn ‚â§ bn) = b1 Za1 ¬∑ ¬∑ ¬∑ b nZ a n p(x1, . . . , xn)dx1 . . . dxn. Thus, a joint density specifies the probability of any joint event over the variables of interest. Both the uniform distribution and the Gaussian distribution have natural extensions to the multivariate case. The definition of a multivariate uniform distribution is straightforward. We defer the definition of the multivariate Gaussian to section 7.1. From the joint density we can derive the marginal density of any random variable by integrating out the other variables. Thus, for example, if p(x, y) is the joint density of X and Y

then p(x) = ‚àû Z ‚àí‚àû p(x, y)dy. To see why this equality holds, note that the event a ‚â§ X ‚â§ b is, by definition, equal to the event ‚Äúa ‚â§ X ‚â§ b and ‚àí‚àû ‚â§ Y ‚â§ ‚àû.‚Äù This rule is the direct analogue of marginalization for discrete variables. Note that, as with discrete probability distributions, we abuse notation a bit and use p to denote both the joint density of X and Y and the marginal density of X. In cases where the distinction is not clear, we use subscripts, so that pX will be the marginal density, of X, and pX,Y the joint density.

6.3 Conditional Density Functions
As with discrete random variables, we want to be able to describe conditional distributions of continuous variables. Suppose, for example, we want to define P(Y | X = x). Applying the definition of conditional distribution (equation (2.1)), we run into a problem, since P(X = x) = 0. Thus, the ratio of P(Y, X = x) and P(X = x) is undefined. To avoid this problem, we might consider conditioning on the event x ‚àí  ‚â§ X ‚â§ x + , which can have a positive probability. Now, the conditional probability is well defined. Thus, we might consider the limit of this quantity when  ‚Üí 0. We define P(Y | x) = lim ‚Üí0 P(Y | x ‚àí  ‚â§ X ‚â§ x + ). When does this limit exist? If there is a continuous joint density function p(x, y), then we can derive the form for this term. To do so, consider some event on Y , say a ‚â§ Y ‚â§ b. Recall that P(a ‚â§ Y ‚â§ b | x ‚àí  ‚â§ X ‚â§ x + ) = P(a ‚â§ Y ‚â§ b, x ‚àí  ‚â§ X ‚â§ x + ) P(x ‚àí  ‚â§ X ‚â§ x + ) = Ra b Rx x‚àí + p(x0, y)dydx0 Rx x‚àí + p(x0)dx0 . When  is suciently small, we can approximate x+ Z x‚àí p(x0)dx0 ‚âà 2p(x). Using a similar approximation for p(x0, y), we get P(a ‚â§ Y ‚â§ b | x ‚àí  ‚â§ X ‚â§ x + ) ‚âà Ra b 2p(x, y)dy 2p(x) = bZa p(x, y) p(x) dy. We conclude that p(x,y) p(x) is the density of P(Y | X = x).

Let p(x, y) be the joint density of X and Y . The conditional density function of Y given X is conditional density function defined as p(y | x) = p(x, y) p(x) When p(x) = 0, the conditional density is undefined. The conditional density p(y | x) characterizes the conditional distribution P(Y | X = x) we defined earlier. The properties of joint distributions and conditional distributions carry over to joint and conditional density functions. In particular, we have the chain rule p(x, y) = p(x)p(y | x) (2.14) and Bayes‚Äô rule p(x | y) = p(x)p(y | x) p(y) . (2.15) As a general statement, whenever we discuss joint distributions of continuous random variables, we discuss properties with respect to the joint density function instead of the joint distribution, as we do in the case of discrete variables. Of particular interest is the notion of (conditional) independence of continuous random variables. Definition 2.10 Let X, Y , and Z be sets of continuous random variables with joint density p(X, Y , Z). We say conditional that X is conditionally independent of Y given Z if independence p(x | z) = p(x | y, z) for all x, y, z such that p(z) > 0.

\section{K√¨ v·ªçng v√† ph∆∞∆°ng sai}

\subsection{K√¨ v·ªçng}

\definition{k√¨ v·ªçng}{
Cho $X$ l√† m·ªôt bi·∫øn r·ªùi r·∫°c nh·∫≠n c√°c gi√° tr·ªã s·ªë, khi ƒë√≥ k√¨ v·ªçng c·ªßa $X$ d∆∞·ªõi ph√¢n ph·ªëi $P$ l√†

$$\mathbb{E}_P[X] = \sum_{x} x \cdot P(x).$$

N·∫øu $X$ l√† m·ªôt bi·∫øn li√™n t·ª•c, khi ƒë√≥ s·ª≠ d·ª•ng h√†m m·∫≠t ƒë·ªô

$$\mathbb{E}_P[X] = \int x \cdot p(x).$$

V√≠ d·ª•, n·∫øu $X$ l√† s·ªë ch·∫•m hi·ªán ra khi tung m·ªôt x√∫c s·∫Øc v·ªõi x√°c su·∫•t xu·∫•t hi·ªán m·ªói m·∫∑t l√† $\frac{1}{6}$. Khi ƒë√≥, k√¨ v·ªçng c·ªßa $\mathbb{E} [X]= 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \cdots + 6 \cdot \frac{1}{6} = 3.5.$ N·∫øu trong tr∆∞·ªùng h·ª£p x√∫c s·∫Øc kh√¥ng c√¢n b·∫±ng $P(X=6) = 0.5$ v√† $P(X=x)=0.1)$ v·ªõi $x < 6$, khi ƒë√≥, $\mathbb{E} [X] = 1 \cdot 0.1 + 2 \cdot 0.1 + \cdots + 5 \cdot 0.1 + 6 \cdots 0.5 = 4.5.$
}


Often we are interested in expectations of a function of a random variable (or several random variables). Thus, we might consider extending the definition to consider the expectation of a functional term such as X2 + 0.5X. Note, however, that any function g of a set of random variables X1, . . . , Xk is essentially defining a new random variable Y : For any outcome œâ ‚àà ‚Ñ¶, we define the value of Y as g(fX1(œâ), . . . , fXk(œâ)). Based on this discussion, we often define new random variables by a functional term. For example Y = X2, or Y = eX. We can also consider functions that map values of one or more categorical random variables to numerical values. One such function that we use quite often is indicator function the indicator function, which we denote 11{X = x}. This function takes value 1 when X = x, and 0 otherwise. In addition, we often consider expectations of functions of random variables without bothering to name the random variables they define. For example IEP [X + Y ]. Nonetheless, we should keep in mind that such a term does refer to an expectation of a random variable. We now turn to examine properties of the expectation of a random variable. First, as can be easily seen, the expectation of a random variable is a linear function in that random variable. Thus, IE[a ¬∑ X + b] = aIE[X] + b. A more complex situation is when we consider the expectation of a function of several random variables that have some joint behavior. An important property of expectation is that the expectation of a sum of two random variables is the sum of the expectations. Proposition 2.4 IE[X + Y ] = IE[X] + IE[Y ]. linearity of This property is called linearity of expectation. It is important to stress that this identity is true expectation even when the variables are not independent. As we will see, this property is key in simplifying many seemingly complex problems. Finally, what can we say about the expectation of a product of two random variables? In general, very little: Example 2.5 Consider two random variables X and Y , each of which takes the value +1 with probability 1/2, and the value ‚àí1 with probability 1/2. If X and Y are independent, then IE[X ¬∑ Y ] = 0. On the other hand, if X and Y are correlated in that they always take the same value, then IE[X ¬∑ Y ] = 1. However, when X and Y are independent, then, as in our example, we can compute the expectation simply as a product of their individual expectations: Proposition 2.5 If X and Y are independent, then IE[X ¬∑ Y ] = IE[X] ¬∑ IE[Y ]. conditional We often also use the expectation given some evidence. The conditional expectation of X expectation given y is IEP [X | y] = X x x ¬∑ P(x | y).

\subsection{Ph∆∞∆°ng sai}

\definition{ph∆∞∆°ng sai}{
K√¨ v·ªçng c·ªßa $X$ ch·ªâ gi√° tr·ªã trung b√¨nh c·ªßa $X$. Tuy nhi√™n, n√≥ kh√¥ng ch·ªâ s·ª± kh√°c nhau gi·ªØa c√°c gi√° tr·ªã m√† $X$ c√≥ th·ªÉ nh·∫≠n.

$$\mathbb{V}ar_P[X] = \mathbb{E}_P \left[(X-\mathbb{E}_P[X])^2\right].$$
}



 A measure of this deviation is the variance of X. VVarP [X] = IEP h(X ‚àí IEP [X])2i. Thus, the variance is the expectation of the squared dierence between X and its expected value. It gives us an indication of the spread of values of X around the expected value. An alternative formulation of the variance is VVar[X] = IEX2 ‚àí (IE[X])2 . (2.16) (see exercise 2.11). Similar to the expectation, we can consider the expectation of a functions of random variables. Proposition 2.6 If X and Y are independent, then VVar[X + Y ] = VVar[X] + VVar[Y ]. It is straightforward to show that the variance scales as a quadratic function of X. In particular, we have: VVar[a ¬∑ X + b] = a2VVar[X]. For this reason, we are often interested in the square root of the variance, which is called the standard standard deviation of the random variable. We define deviation œÉX = pVVar[X]. The intuition is that it is improbable to encounter values of X that are farther than several standard deviations from the expected value of X. Thus, œÉX is a normalized measure of ‚Äúdistance‚Äù from the expected value of X. As an example consider the Gaussian distribution of definition 2.7. Proposition 2.7 Let X be a random variable with Gaussian distribution N(¬µ, œÉ2), then IE[X] = ¬µ and VVar[X] = œÉ2. Thus, the parameters of the Gaussian distribution specify the expectation and the variance of the distribution. As we can see from the form of the distribution, the density of values of X drops exponentially fast in the distance x‚àí¬µ œÉ . Not all distributions show such a rapid decline in the probability of outcomes that are distant from the expectation. However, even for arbitrary distributions, one can show that there is a decline. Theorem 2.1 (Chebyshev inequality): Chebyshev‚Äôs inequality P (|X ‚àí IEP [X]| ‚â• t) ‚â§ VVarP [X] t2 .

We can restate this inequality in terms of standard deviations: We write t = kœÉX to get P(|X ‚àí IEP [X]| ‚â• kœÉX) ‚â§ 1 k2. Thus, for example, the probability of X being more than two standard deviations away from IE[X] is less than 1/4.

\section{C√°c h√†m ph√¢n ph·ªëi th√¥ng d·ª•ng}

Ph·∫ßn n√†y c√≥ th√™m kh·∫£o \cite{Goodfellow-et-al-2016} v√† gi√°o tr√¨nh x√°c su·∫•t th·ªëng k√™ c·ªßa th·∫°c s·ªπ Tr·∫ßn Thi·ªán Kh·∫£i, ƒë·∫°i h·ªçc Tr√† Vinh  \footnote{\href{http://www.ctec.tvu.edu.vn/ttkhai/xacsuatthongke_dh.htm}{http://www.ctec.tvu.edu.vn/ttkhai/xacsuatthongke\_dh.htm}}

\diary{17/01/2018 L√≤ng v√≤ng th·∫ø n√†o h√¥m nay l·∫°i t√¨m ƒë∆∞·ª£c  \href{https://dominhhai.github.io/vi/2017/10/prob-com-var}{blog c·ªßa b·∫°n ƒê·ªó Minh H·∫£i}, r·∫•t hay
}
\subsection{Bi·∫øn r·ªùi r·∫°c}

T·ªïng k·∫øt c√°c ph√¢n ph·ªëi r·ªùi r·∫°c tham kh·∫£o slide 3, chapter 9 c·ªßa kh√≥a CS 109 \footnote{\href{https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/ppt/9-ContinuousDist.pdf}{Slide 3, chapter 9, CS109 Stanford}}

\textbf{Ph√¢n ph·ªëi Bernoulli:}

\begin{itemize}
  \item x√°c su·∫•t xu·∫•t hi·ªán m·∫∑t ng·ª≠a $X \sim Ber(p)$
\end{itemize}

\textbf{Ph√¢n ph·ªëi Bernoulli:}

\begin{itemize}
  \item x√°c su·∫•t xu·∫•t hi·ªán m·∫∑t ng·ª≠a $X \sim Ber(p)$
\end{itemize}

\textbf{Ph√¢n ph·ªëi Binomial:}

\begin{itemize}
  \item x√°c su·∫•t th√†nh c√¥ng n l·∫ßn $X \sim B(n,p)$
\end{itemize}

\textbf{Ph√¢n ph·ªëi Poisson:}

\begin{itemize}
  \item x√°c su·∫•t th√†nh c√¥ng n l·∫ßn $X \sim Poi(\lambda)$
\end{itemize}

\textbf{Ph√¢n ph·ªëi Geometric:}

\begin{itemize}
  \item s·ªë l·∫ßn th·ª≠ ƒë·∫øn khi th√†nh c√¥ng $X \sim Geo(p)$
\end{itemize}

\textbf{Ph√¢n ph·ªëi Negative Binomial:}

\begin{itemize}
  \item s·ªë l·∫ßn th·ª≠ ƒë·∫øn khi $r$ th√†nh c√¥ng $X \sim NegBin(r,p)$
\end{itemize}

\textbf{Ph√¢n ph·ªëi Hyper Geometric:}

\begin{itemize}
  \item s·ªë b√≥ng tr·∫Øng l·∫•y ƒë∆∞·ª£c trong $N$ b√≥ng ch·ª©a $m$ b√≥ng tr·∫Øng  $X \sim HypG(n, N, m)$
\end{itemize}

\subsection{Bi·∫øn ng·∫´u nhi√™n Bernoulli}

\textbf{Ph√©p th·ª≠ Bernoulli} l√† m·ªôt ph√©p th·ª≠ ch·ªâ c√≥ hai kh·∫£ nƒÉng x·∫£y ra "th√†nh c√¥ng" ho·∫∑c "th·∫•t b·∫°i". Trong th·ª±c t·∫ø, c√°c v√≠ d·ª• c·ªßa ph√©p th·ª≠ Bernoulli xu·∫•t hi·ªán r·∫•t ph·ªï bi·∫øn nh∆∞ \textit{tung ƒë·ªìng xu}, \textit{s·ª± ki·ªán m·ªôt ·ªï ƒëƒ©a b·ªã h·ªèng}, \textit{s·ª± ki·ªán m·ªôt ai ƒë√≥ th√≠ch xem m·ªôt b·ªô phim tr√™n Netflix}.
i√™n
ƒê·ªãnh nghƒ©a X l√† m·ªôt bi·∫øn ng·∫´u nhi√™n, nh·∫≠n gi√° tr·ªã b·∫±ng 1 n·∫øu s·ª± ki√™n th√†nh c√¥ng, nh·∫≠n gi√° tr·ªã b·∫±ng 0 n·∫øu s·ª± ki·ªán th·∫•t b·∫°i.

X√°c su·∫•t s·ª± ki·ªán th√†nh c√¥ng (X nh·∫≠n gi√° tr·ªã b·∫±ng 1) l√† $P(X = 1) = p(1) = p$, x√°c su·∫•t s·ª± ki√™n th·∫•t b·∫°i $P(X = 0) = p(0) = 1 - p$

\textbf{X l√† bi·∫øn ng·∫´u nhi√™n Bernoulli}

$$X \sim Ber(p)$$

Khi ƒë√≥, k√¨ v·ªçng c·ªßa X

$$E[X] = p$$

Ph∆∞∆°ng sai c·ªßa X

$$Var(X)=p(1-p)$$

\subsection{Ph√¢n ph·ªëi Bernoulli}

Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p v·ªÅ ph√©p th·ª≠ B√©c-nu-li r·∫±ng m·ªçi ph√©p th·ª≠ c·ªßa n√≥ ch·ªâ cho 2 k·∫øt qu·∫£ duy nh·∫•t l√† $A$ v·ªõi x√°c su·∫•t $p$ v√† $\bar A$ v·ªõi x√°c su·∫•t $q=1-p$
Bi·∫øn ng·∫´u nhi√™n $X$ tu√¢n theo ph√¢n ph·ªëi B√©c-nu-li
$$X \sim B(p)$$
v·ªõi tham s·ªë $p \in \mathbb{R}, 0 \leq p \leq 1$ l√† x√°c su·∫•t xu·∫•t hi·ªán c·ªßa $A$ t·∫°i m·ªói ph√©p th·ª≠
\newline
\begin{tabular}{ | l | l | l | }
  \hline
  ƒê·ªãnh nghƒ©a & & Gi√° tr·ªã \\
  \hline
  PMF & p(x) & $p(x)$ | $p^x (1-p)^{1-x}, x \in \{0, 1\} $ \\
  \hline
  CDF & $F(x;p)$  &
  \begin{cases}
    0 & \text{for } x < 0 \\
    1-p & \text{for } 0 \leq x < 1 \\
    1 & \text{for } x \geq 1
  \end{cases} \\
  \hline
  K·ª≥ v·ªçng & $E[X]$ & $p$ \\
  \hline
  Ph∆∞∆°ng sai & $Var(X)$ & $p(1-p)$ \\
  \hline
\end{tabular}

\subsection{Ph√¢n ph·ªëi Binomial}

\begin{itemize}
  \item x√°c su·∫•t th√†nh c√¥ng n l·∫ßn $X \sim B(n,p)$
\end{itemize}

\subsection{Ph√¢n ph·ªëi Poisson}

\begin{itemize}
  \item x√°c su·∫•t th√†nh c√¥ng n l·∫ßn $X \sim Poi(\lambda)$
\end{itemize}

\subsection{Ph√¢n ph·ªëi Geometric}

\begin{itemize}
  \item s·ªë l·∫ßn th·ª≠ ƒë·∫øn khi th√†nh c√¥ng $X \sim Geo(p)$
\end{itemize}

\subsection{Ph√¢n ph·ªëi Negative Binomial}

\begin{itemize}
  \item s·ªë l·∫ßn th·ª≠ ƒë·∫øn khi $r$ th√†nh c√¥ng $X \sim NegBin(r,p)$
\end{itemize}

\subsection{Ph√¢n ph·ªëi Hyper Geometric}

\begin{itemize}
  \item s·ªë b√≥ng tr·∫Øng l·∫•y ƒë∆∞·ª£c trong $N$ b√≥ng ch·ª©a $m$ b√≥ng tr·∫Øng  $X \sim HypG(n, N, m)$
\end{itemize}

\subsection{Bi·∫øn li√™n t·ª•c}

\subsection{Ph√¢n ph·ªëi ƒë·ªÅu}

\definition{ph√¢n ph·ªëi ƒë·ªÅu}{
L√† ph√¢n ph·ªëi m√† x√°c su·∫•t xu·∫•t hi·ªán c·ªßa c√°c s·ª± ki·ªán l√† nh∆∞ nhau.
}

\newline

Bi·∫øn ng·∫´u nhi√™n $X$ tu√¢n theo ph√¢n ph·ªëi ƒë·ªÅu r·ªùi r·∫°c

$$X \sim Unif (a, b)$$

v·ªõi tham s·ªë $a, b \in \mathbb Z; a < b$ l√† kho·∫£ng gi√° tr·ªã c·ªßa $X$, ƒë·∫∑t $n = b-a+1$
\newline
Ta s·∫Ω c√≥:
\newline
\begin{tabular}[c]{ | l | l | }
  \hline
  ƒê·ªãnh nghƒ©a & Gi√° tr·ªã \\
  \hline
  PMF & $p(x)$ | $\dfrac{1}{n}, \forall x \in [a,b]$ \\
  \hline
  CDF - $F(x;a,b)$ & $\dfrac{x-a+1}{n}, \forall x \in [a,b]$ \\
  \hline
  K·ª≥ v·ªçng - $E[X]$ & $\dfrac{a+b}{2}$ \\
  \hline
  Ph∆∞∆°ng sai - $Var(X)$ & $\dfrac{n^2-1}{12}$ \\
  \hline
\end{tabular}

\newline

\noindent V√≠ d·ª• 1:
\index{b√†i to√°n ch·ªù xe bu√Ωt}

\definition{b√†i to√°n ch·ªù xe bu√Ωt}{
L·ªãch ch·∫°y c·ªßa xe bu√Ωt t·∫°i m·ªôt tr·∫°m xe bu√Ωt nh∆∞ sau: chi·∫øc xe bu√Ωt ƒë·∫ßu ti√™n trong ng√†y s·∫Ω kh·ªüi h√†nh t·ª´ tr·∫°m n√†y v√†o l√∫c 7 gi·ªù, c·ª© sau m·ªói 15 ph√∫t s·∫Ω c√≥ m·ªôt xe kh√°c ƒë·∫øn tr·∫°m. Gi·∫£ s·ª≠ m·ªôt h√†nh kh√°ch ƒë·∫øn tr·∫°m trong kho·∫£ng th·ªùi gian t·ª´ 7 gi·ªù ƒë·∫øn 7 gi·ªù 30. T√¨m x√°c su·∫•t ƒë·ªÉ h√†nh kh√°ch n√†y ch·ªù:

a) √çt h∆°n 5 ph√∫t.

b) √çt nh·∫•t 12 ph√∫t.

}

\textbf{Gi·∫£i}

G·ªçi X l√† s·ªë ph√∫t sau 7 gi·ªù m√† h√†nh kh√°ch ƒë·∫øn tr·∫°m.

Ta c√≥: $X \sim R[0;30]$.

a) H√†nh kh√°ch s·∫Ω ch·ªù √≠t h∆°n 5 ph√∫t n·∫øu ƒë·∫øn tr·∫°m gi·ªØa 7 gi·ªù 10 v√† 7 gi·ªù 15 ho·∫∑c gi·ªØa 7 gi·ªù 25 v√† 7 gi·ªù 30. Do ƒë√≥ x√°c su·∫•t c·∫ßn t√¨m l√†:

$$P(0<X<15) + P(25<X<30)=\frac{5}{30} + \frac{5}{30}=\frac{1}{3}$$

b) H√†nh kh√°ch ch·ªù √≠t nh·∫•t 12 ph√∫t n·∫øu ƒë·∫øn tr·∫°m gi·ªØa 7 gi·ªù v√† 7 gi·ªù 3 ph√∫t ho·∫∑c gi·ªØa 7 gi·ªù 15 ph√∫t v√† 7 gi·ªù 18 ph√∫t. X√°c su·∫•t c·∫ßn t√¨m l√†:

$$P(0<X<3) + P(15<X<18)=\frac{3}{30} + \frac{3}{30}=\frac{1}{5}$$
